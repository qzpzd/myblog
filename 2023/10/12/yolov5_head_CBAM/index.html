<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力 | 两个世界的我</title><meta name="author" content="guzhongyue666"><meta name="copyright" content="guzhongyue666"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="1.CBAM介绍论文题目：《CBAM: Convolutional Block Attention Module》论文地址：  https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1807.06521.pdf实验证明，将CBAM注意力模块嵌入到YOLOv5网络中，有利于解决原始网络无注意力偏好的问题。主要在分类问题中比较明显  CBAM注意力结构基本原理：从上图明显可以看到， CBAM一共包含2个独立的子">
<meta property="og:type" content="article">
<meta property="og:title" content="(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力">
<meta property="og:url" content="https://qzpzd.github.io/myblog/2023/10/12/yolov5_head_CBAM/index.html">
<meta property="og:site_name" content="两个世界的我">
<meta property="og:description" content="1.CBAM介绍论文题目：《CBAM: Convolutional Block Attention Module》论文地址：  https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1807.06521.pdf实验证明，将CBAM注意力模块嵌入到YOLOv5网络中，有利于解决原始网络无注意力偏好的问题。主要在分类问题中比较明显  CBAM注意力结构基本原理：从上图明显可以看到， CBAM一共包含2个独立的子">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/10/12/wUIdgiNVbkvB6ap.png">
<meta property="article:published_time" content="2023-10-12T06:51:35.000Z">
<meta property="article:modified_time" content="2023-10-12T09:28:00.052Z">
<meta property="article:author" content="guzhongyue666">
<meta property="article:tag" content="-yolov5 -head -CBAM -ODConv">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/10/12/wUIdgiNVbkvB6ap.png"><link rel="shortcut icon" href="/myblog/img/favicon.jpg"><link rel="canonical" href="https://qzpzd.github.io/myblog/2023/10/12/yolov5_head_CBAM/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/myblog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/myblog/',
  algolia: undefined,
  localSearch: {"path":"/myblog/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-12 17:28:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/myblog//css/mycss.css"><style type="text/css">.aplayer.aplayer-narrow .aplayer-body{left: -66px !important;transition: left 1s;} .aplayer.aplayer-narrow .aplayer-body:hover{left: 0 !important;}</style><link rel="stylesheet" href="APlayer.min.css"><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/gh/radium-bit/res@master/live2d/autoload.js" async></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js" async></script><meta name="generator" content="Hexo 7.0.0-rc2"><link rel="alternate" href="/myblog/atom.xml" title="两个世界的我" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/myblog/img/favicon.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/myblog/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/myblog/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/myblog/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/myblog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/myblog/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/myblog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/myblog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/myblog/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/myblog/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/myblog/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/myblog/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/myblog/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/10/12/wUIdgiNVbkvB6ap.png')"><nav id="nav"><span id="blog-info"><a href="/myblog/" title="两个世界的我"><span class="site-name">两个世界的我</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/myblog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/myblog/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/myblog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/myblog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/myblog/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/myblog/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/myblog/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/myblog/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/myblog/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-12T06:51:35.000Z" title="发表于 2023-10-12 14:51:35">2023-10-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-10-12T09:28:00.052Z" title="更新于 2023-10-12 17:28:00">2023-10-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/myblog/2023/10/12/yolov5_head_CBAM/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/myblog/2023/10/12/yolov5_head_CBAM/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="1-CBAM介绍"><a href="#1-CBAM介绍" class="headerlink" title="1.CBAM介绍"></a>1.CBAM介绍</h1><p>论文题目：《CBAM: Convolutional Block Attention Module》<br>论文地址：  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.06521.pdf">https://arxiv.org/pdf/1807.06521.pdf</a><br>实验证明，将CBAM注意力模块嵌入到YOLOv5网络中，有利于解决原始网络无注意力偏好的问题。主要在分类问题中比较明显</p>
<p><img src="https://s2.loli.net/2023/10/12/wUIdgiNVbkvB6ap.png" alt="image.png"></p>
<p>CBAM注意力结构基本原理：从上图明显可以看到， CBAM一共包含2个独立的子模块， 通道注意力模块（Channel Attention Module，CAM) 和空间注意力模块（Spartial Attention Module，SAM) ，分别进行通道与空间维度上的注意力特征融合。 这样不只能够节约参数和计算力，并且保证了其能够做为即插即用的模块集成到现有的网络架构中去。</p>
<p>那对应YOLOv5结合CBAM需要修改哪些地方：</p>
<h1 id="2-common-py中加入CBAM代码"><a href="#2-common-py中加入CBAM代码" class="headerlink" title="2.common.py中加入CBAM代码"></a>2.common.py中加入CBAM代码</h1><figure class="highlight plaintext"><figcaption><span>[CBAM]</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class ChannelAttentionModule(nn.Module):<br>    def __init__(self, c1, reduction=16):<br>        super(ChannelAttentionModule, self).__init__()<br>        mid_channel = c1 // reduction<br>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<br>        self.max_pool = nn.AdaptiveMaxPool2d(1)<br>        self.shared_MLP = nn.Sequential(<br>            nn.Linear(in_features=c1, out_features=mid_channel),<br>            nn.ReLU(),<br>            nn.Linear(in_features=mid_channel, out_features=c1)<br>        )<br>        self.sigmoid = nn.Sigmoid()<br>        #self.act=SiLU()<br>    def forward(self, x):<br>        avgout = self.shared_MLP(self.avg_pool(x).view(x.size(0),-1)).unsqueeze(2).unsqueeze(3)<br>        maxout = self.shared_MLP(self.max_pool(x).view(x.size(0),-1)).unsqueeze(2).unsqueeze(3)<br>        return self.sigmoid(avgout + maxout)<br>class SpatialAttentionModule(nn.Module):<br>    def __init__(self):<br>        super(SpatialAttentionModule, self).__init__()<br>        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3) <br>        #self.act=SiLU()<br>        self.sigmoid = nn.Sigmoid()<br>    def forward(self, x):<br>        avgout = torch.mean(x, dim=1, keepdim=True)<br>        maxout, _ = torch.max(x, dim=1, keepdim=True)<br>        out = torch.cat([avgout, maxout], dim=1)<br>        out = self.sigmoid(self.conv2d(out))<br>        return out<br> <br>class CBAM(nn.Module):<br>    def __init__(self, c1,c2):<br>        super(CBAM, self).__init__()<br>        self.channel_attention = ChannelAttentionModule(c1)<br>        self.spatial_attention = SpatialAttentionModule()<br> <br>    def forward(self, x):<br>        out = self.channel_attention(x) * x<br>        out = self.spatial_attention(out) * out<br>        return out<br></code></pre></td></tr></table></figure>


<h1 id="3-在yolo-py文件中添加对应的CBAM"><a href="#3-在yolo-py文件中添加对应的CBAM" class="headerlink" title="3.在yolo.py文件中添加对应的CBAM"></a>3.在yolo.py文件中添加对应的CBAM</h1><figure class="highlight plaintext"><figcaption><span>[yolo.py]</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"> if m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, <br>DWConv, MixConv2d, Focus, CrossConv, BottleneckCSP, C3, C3TR, CBAM]:      <br></code></pre></td></tr></table></figure>

<h1 id="4-在yolov5s-yaml文件中添加CBAM"><a href="#4-在yolov5s-yaml文件中添加CBAM" class="headerlink" title="4.在yolov5s.yaml文件中添加CBAM"></a>4.在yolov5s.yaml文件中添加CBAM</h1><p>根据实际训练效果，在配置文件中的C3模块后面适当添加CBAM注意力模块，过程中注意通道数和网络层数的变化（注：不同添加位置效果可能不大一样），例如下面作为参考：在最后检测层上面添加CBAM，其中BiFPND，BiFPNT分别是二分支与三分支的BiFPN</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">#Head<br>head:<br>  [[-1, 1, Conv, [512, 1, 1]],                     #14<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],     #15<br>   [[-1, 9], 1, BiFPND, [256, 256]],               #16 <br>   [-1, 3, C3, [512, False]],                      #17<br> <br>   [-1, 1, Conv, [256, 1, 1]],                     #18<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],     #19<br>   [[-1, 6], 1, BiFPND, [128, 128]],               #20 <br>   [-1, 3, C3, [256, False]],                      #21 <br> <br>   [-1, 1, Conv, [512, 3, 2]],                     #22<br>   [[-1, 17, 9], 1, BiFPNT, [256, 256]],          #23<br>   [-1, 3, C3, [512, False]],                      #24 <br> <br>   [-1, 1, Conv, [512, 3, 2]],                     #25<br>   [[-1, 14], 1, BiFPND, [256, 256]],              #26<br>   [-1, 3, C3, [1024, False]],                     #27 <br>   [-1, 1, CBAM, [1024]],                            #28<br> <br>   [[21, 24, 28], 1, Detect, [nc, anchors]],       #29 Detect<br>  ]<br></code></pre></td></tr></table></figure>

<h1 id="其它注意力SE，ECA与CSBAM区别"><a href="#其它注意力SE，ECA与CSBAM区别" class="headerlink" title="其它注意力SE，ECA与CSBAM区别"></a>其它注意力SE，ECA与CSBAM区别</h1><p>注意力机制顾名思义就是通过对感兴趣的区域提升更多的注意力，尽可能的抑制不感兴趣的区域在图像分割中的作用。深度学习CNN中可以将注意力机制分为通道注意力和空间注意力两种，通道注意力是确定不同通道之间的权重关系，提升重点通道的权重，抑制作用不大的通道，空间注意力是确定空间邻域不同像素之间的权重关系，提升重点区域像素的权重，让算法更多的关注我们需要的研究区域，减小非必要区域的权重。</p>
<p><strong>一、SE (Squeeze and Excitation）注意力机制</strong><br>SE注意力机制是通道注意力模式下的一种确定权重的方法，它通过在不同通道间分配权重达到主次优先的目的。如下图所示，为SE注意力机制的结构图。</p>
<p>该结构主要分为以下三个方面：</p>
<p>①：通过将特征图进行Squeeze(压缩)，该步骤是通过全局平均池化把特征图从大小为（N,C,H,W)转换为（N,C,1,1)，这样就达到了全局上下文信息的融合。</p>
<p>②：Excitation操作，该步骤使用两个全连接层，通过全连接层之间的非线性添加模型的复杂度，达到确定不同通道之间的权重作用，其中第一个全连接层使用ReLU激活函数，第二个全连接层采用Sigmoid激活函数，为了将权重中映射到（0，1）之间。值得注意的是，为了减少计算量进行降维处理，将第一个全连接的输出采用输入的1&#x2F;4或者1&#x2F;16。</p>
<p>③：将reshape过后的权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。</p>
<p><img src="https://s2.loli.net/2023/10/12/IPld8LETkFc6B5w.png" alt="image.png"></p>
<p>具体PyTorch实现代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import torch<br>import torch.nn as nn<br><br><br>class Se(nn.Module):<br>    def __init__(self,in_channel,reduction=16):<br>        super(Se, self).__init__()<br>        self.pool=nn.AdaptiveAvgPool2d(output_size=1)<br>        self.fc=nn.Sequential(<br>            nn.Linear(in_features=in_channel,out_features=in_channel//reduction,bias=False),<br>            nn.ReLU(),<br>            nn.Linear(in_features=in_channel//reduction,out_features=in_channel,bias=False),<br>            nn.Sigmoid()<br>        )<br><br>    def forward(self,x):<br>        out=self.pool(x)<br>        out=self.fc(out.view(out.size(0),-1))<br>        out=out.view(x.size(0),x.size(1),1,1)<br>        return out*x<br></code></pre></td></tr></table></figure>

<p><strong>二、ECA（Efficient Channel Attention）</strong><br>ECA注意力机制也是通道注意力的一种方法，该算法是在SE算法的基础上做出了一定的改进，首先ECA作者认为SE虽然全连接的降维可以降低模型的复杂度，但是破坏了通道与其权重之间的直接对应关系，先降维后升维，这样权重和通道的对应关系是间接的，基于上述，作者提出一维卷积的方法，避免了降维对数据的影响。</p>
<p>该结构主要分为以下三个方面：</p>
<p>①：通过将特征图进行Squeeze(压缩)，该步骤是通过全局平均池化把特征图从大小为（N,C,H,W)转换为（N,C,1,1)，这样就达到了全局上下文信息的融合，该步骤和SE一样。</p>
<p>②：计算自适应卷积核的大小，<br> ，其中C为输入的通道数，b&#x3D;1，<br> &#x3D;2，并采用一维卷积计算通道的权重，最后采用Sigmoid激活函数将权重映射在（0-1）之间。</p>
<p>③：将reshape过后的权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。</p>
<p><img src="https://s2.loli.net/2023/10/12/NhXnP3lx5a7FTvG.png" alt="image.png"></p>
<p>具体PyTorch实现代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import torch<br>import torch.nn as nn<br>import math<br>class ECA(nn.Module):<br>    def __init__(self,in_channel,gamma=2,b=1):<br>        super(ECA, self).__init__()<br>        k=int(abs((math.log(in_channel,2)+b)/gamma))<br>        kernel_size=k if k % 2 else k+1<br>        padding=kernel_size//2<br>        self.pool=nn.AdaptiveAvgPool2d(output_size=1)<br>        self.conv=nn.Sequential(<br>            nn.Conv1d(in_channels=1,out_channels=1,kernel_size=kernel_size,padding=padding,bias=False),<br>            nn.Sigmoid()<br>        )<br><br>    def forward(self,x):<br>        out=self.pool(x)<br>        out=out.view(x.size(0),1,x.size(1))<br>        out=self.conv(out)<br>        out=out.view(x.size(0),x.size(1),1,1)<br>        return out*x<br></code></pre></td></tr></table></figure>

<p><strong>三、CBAM（Convolutional Block Attention Module）</strong><br>CBAM注意力机制是一种将通道与空间注意力机制相结合的算法模型，算法整体结构如图3所示，输入特征图先进行通道注意力机制再进行空间注意力机制操作，最后输出，这样从通道和空间两个方面达到了强化感兴趣区域的目的。</p>
<p>通道结构主要分为以下三个方面：</p>
<p>①：通过将特征图进行Squeeze(压缩)，该步骤分别采用全局平均池化和全局最大池化把特征图从大小为（N,C,H,W)转换为（N,C,1,1)，这样就达到了全局上下文信息的融合。</p>
<p>②：分别将全局最大池化和全局平均池化结果进行MLP（多层感知机）操作，MLP在这里定义与SE的操作一样，为两层全连接层，中间采用ReLU激活，最后将两者相加后利用Sigmoid函数激活。</p>
<p>③：将reshape过后的权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。</p>
<p>空间结构主要分为以下三个方面：</p>
<p>①：将上述通道注意力操作的结果，分别在通道维度上进行最大池化和平均池化，即将经过通道注意力机制的特征图从（N,C,H,W)转换为（N,1,H,W），达到融合不同通道的信息的效果，然后在通道维度上将最大池化与平均池化结果叠加起来，即采用torch.cat()。</p>
<p>②：将叠加后2个通道的结果做卷积运算，输出通道为1，卷积核大小为7，最后将输出结果采用Sigmoid函数激活。</p>
<p>③：将权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。</p>
<p><img src="https://s2.loli.net/2023/10/12/wGMIACQ87LpNdhg.png" alt="image.png"></p>
<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import torch<br>import torch.nn as nn<br>import math<br>class CBAM(nn.Module):<br>    def __init__(self,in_channel,reduction=16,kernel_size=7):<br>        super(CBAM, self).__init__()<br>        #通道注意力机制<br>        self.max_pool=nn.AdaptiveMaxPool2d(output_size=1)<br>        self.avg_pool=nn.AdaptiveAvgPool2d(output_size=1)<br>        self.mlp=nn.Sequential(<br>            nn.Linear(in_features=in_channel,out_features=in_channel//reduction,bias=False),<br>            nn.ReLU(),<br>            nn.Linear(in_features=in_channel//reduction,out_features=in_channel,bias=False)<br>        )<br>        self.sigmoid=nn.Sigmoid()<br>        #空间注意力机制<br>        self.conv=nn.Conv2d(in_channels=2,out_channels=1,kernel_size=kernel_size ,stride=1,padding=kernel_size//2,bias=False)<br><br>    def forward(self,x):<br>        #通道注意力机制<br>        maxout=self.max_pool(x)<br>        maxout=self.mlp(maxout.view(maxout.size(0),-1))<br>        avgout=self.avg_pool(x)<br>        avgout=self.mlp(avgout.view(avgout.size(0),-1))<br>        channel_out=self.sigmoid(maxout+avgout)<br>        channel_out=channel_out.view(x.size(0),x.size(1),1,1)<br>        channel_out=channel_out*x<br>        #空间注意力机制<br>        max_out,_=torch.max(channel_out,dim=1,keepdim=True)<br>        mean_out=torch.mean(channel_out,dim=1,keepdim=True)<br>        out=torch.cat((max_out,mean_out),dim=1)<br>        out=self.sigmoid(self.conv(out))<br>        out=out*channel_out<br>        return out<br></code></pre></td></tr></table></figure>

<h1 id="另一利用注意力模块的ODConv：即插即用的动态卷积"><a href="#另一利用注意力模块的ODConv：即插即用的动态卷积" class="headerlink" title="另一利用注意力模块的ODConv：即插即用的动态卷积"></a>另一利用注意力模块的ODConv：即插即用的动态卷积</h1><p>一定程度上讲，ODConv可以视作CondConv的延续，将CondConv中一个维度上的动态特性进行了扩展，同时了考虑了空域、输入通道、输出通道等维度上的动态性，故称之为全维度动态卷积。ODConv通过并行策略采用多维注意力机制沿核空间的四个维度学习互补性注意力。作为一种“即插即用”的操作，它可以轻易的嵌入到现有CNN网络中。ImageNet分类与COCO检测任务上的实验验证了所提ODConv的优异性：即可提升大模型的性能，又可提升轻量型模型的性能</p>
<p>动态卷积这几年研究的非常多了，比较知名的有谷歌的CondConv，旷视科技的WeightNet、MSRA的DynamicConv、华为的DyNet、商汤的CARAFE等<br>常规卷积只有一个静态卷积核且与输入样本无关。对于动态卷积来说，它对多个卷积核进行线性加权，而加权值则与输入有关，这就使得动态卷积具有输入依赖性。它可以描述如下：<br><img src="https://s2.loli.net/2023/10/12/nTFJOcZfz25ML3K.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2023/10/12/CgOm5r9Lqs7TY8K.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2023/10/12/mnjYfTA8EikboZx.png" alt="image.png"></p>
<p>ODConv比一般的动态卷积效果都要好，比一般的注意力机制效果也要好，其实动态卷积与注意力机制是从模块中不同角度进行解析，动态卷积是相比较于常规卷积，最终需要与输入相乘，与输入有关，而常规卷积与输入无关；注意力机制是相对于特征图的四个维度比较的，也即卷积方式不一样，以往是直接进行特征图与卷积核进行卷积来获取特征，而注意力机制就是将特征图分别对四个通道进行压缩，来获取对比某一通道在特征图的影响力，再分别与卷积核进行线性加权，最后与输入相乘。因此像动态卷积与注意力其实一个东西，只是ODConv是考虑到所有维度的注意力，因此效果最好</p>
<p>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/468466504">https://zhuanlan.zhihu.com/p/468466504</a></p>
<h1 id="5-在yolov5中添加ODConv"><a href="#5-在yolov5中添加ODConv" class="headerlink" title="5.在yolov5中添加ODConv"></a>5.在yolov5中添加ODConv</h1><p><strong>1.首先在common.py文件中添加代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>import torch<br>import torch.nn as nn<br>import torch.nn.functional as F<br>import torch.autograd<br><br>class ODConv(nn.Sequential):<br>    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1, norm_layer=nn.BatchNorm2d,<br>                 reduction=0.0625, kernel_num=1):<br>        padding = (kernel_size - 1) // 2<br>        super(ODConv, self).__init__(<br>            ODConv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups,<br>                     reduction=reduction, kernel_num=kernel_num),<br>            norm_layer(out_planes),<br>            nn.SiLU()<br>        )<br><br>class Attention(nn.Module):<br>    def __init__(self, in_planes, out_planes, kernel_size, <br>    groups=1, <br>    reduction=0.0625, <br>    kernel_num=4, <br>    min_channel=16):<br>        super(Attention, self).__init__()<br>        attention_channel = max(int(in_planes * reduction), min_channel)<br>        self.kernel_size = kernel_size<br>        self.kernel_num = kernel_num<br>        self.temperature = 1.0<br><br>        self.avgpool = nn.AdaptiveAvgPool2d(1)<br>        self.fc = nn.Conv2d(in_planes, attention_channel, 1, bias=False)<br>        self.bn = nn.BatchNorm2d(attention_channel)<br>        self.relu = nn.ReLU(inplace=True)<br><br>        self.channel_fc = nn.Conv2d(attention_channel, in_planes, 1, bias=True)<br>        self.func_channel = self.get_channel_attention<br><br>        if in_planes == groups and in_planes == out_planes:  # depth-wise convolution<br>            self.func_filter = self.skip<br>        else:<br>            self.filter_fc = nn.Conv2d(attention_channel, out_planes, 1, bias=True)<br>            self.func_filter = self.get_filter_attention<br><br>        if kernel_size == 1:  # point-wise convolution<br>            self.func_spatial = self.skip<br>        else:<br>            self.spatial_fc = nn.Conv2d(attention_channel, kernel_size * kernel_size, 1, bias=True)<br>            self.func_spatial = self.get_spatial_attention<br><br>        if kernel_num == 1:<br>            self.func_kernel = self.skip<br>        else:<br>            self.kernel_fc = nn.Conv2d(attention_channel, kernel_num, 1, bias=True)<br>            self.func_kernel = self.get_kernel_attention<br>        self.bn_1 = nn.LayerNorm([attention_channel,1,1])<br>        self._initialize_weights()<br><br>    def _initialize_weights(self):<br>        for m in self.modules():<br>            if isinstance(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, mode=&#x27;fan_out&#x27;, nonlinearity=&#x27;relu&#x27;)<br>                if m.bias is not None:<br>                    nn.init.constant_(m.bias, 0)<br>            if isinstance(m, nn.BatchNorm2d):<br>                nn.init.constant_(m.weight, 1)<br>                nn.init.constant_(m.bias, 0)<br><br>    def update_temperature(self, temperature):<br>        self.temperature = temperature<br><br>    @staticmethod<br>    def skip(_):<br>        return 1.0<br><br>    def get_channel_attention(self, x):<br>        channel_attention = torch.sigmoid(self.channel_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)<br>        return channel_attention<br><br>    def get_filter_attention(self, x):<br>        filter_attention = torch.sigmoid(self.filter_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)<br>        return filter_attention<br><br>    def get_spatial_attention(self, x):<br>        spatial_attention = self.spatial_fc(x).view(x.size(0), 1, 1, 1, self.kernel_size, self.kernel_size)<br>        spatial_attention = torch.sigmoid(spatial_attention / self.temperature)<br>        return spatial_attention<br><br>    def get_kernel_attention(self, x):<br>        kernel_attention = self.kernel_fc(x).view(x.size(0), -1, 1, 1, 1, 1)<br>        kernel_attention = F.softmax(kernel_attention / self.temperature, dim=1)<br>        return kernel_attention<br><br>    def forward(self, x):<br>        x = self.avgpool(x)<br>        x = self.fc(x)<br>        x = self.bn_1(x)<br>        x = self.relu(x)<br>        return self.func_channel(x), self.func_filter(x), self.func_spatial(x), self.func_kernel(x)<br><br>class ODConv2d(nn.Module):<br>    def __init__(self, <br>    in_planes, <br>    out_planes, <br>    kernel_size=3, <br>    stride=1, <br>    padding=0, <br>    dilation=1, <br>    groups=1,<br>    reduction=0.0625, <br>    kernel_num=1):<br>        super(ODConv2d, self).__init__()<br>        self.in_planes = in_planes<br>        self.out_planes = out_planes<br>        self.kernel_size = kernel_size<br>        self.stride = stride<br>        self.padding = padding<br>        self.dilation = dilation<br>        self.groups = groups<br>        self.kernel_num = kernel_num<br>        self.attention = Attention(in_planes, out_planes, kernel_size, groups=groups,<br>                                   reduction=reduction, kernel_num=kernel_num)<br>        self.weight = nn.Parameter(torch.randn(kernel_num, out_planes, in_planes//groups, kernel_size, kernel_size),<br>                                   requires_grad=True)<br>        self._initialize_weights()<br><br>        if self.kernel_size == 1 and self.kernel_num == 1:<br>            self._forward_impl = self._forward_impl_pw1x<br>        else:<br>            self._forward_impl = self._forward_impl_common<br><br>    def _initialize_weights(self):<br>        for i in range(self.kernel_num):<br>            nn.init.kaiming_normal_(self.weight[i], mode=&#x27;fan_out&#x27;, nonlinearity=&#x27;relu&#x27;)<br><br>    def update_temperature(self, temperature):<br>        self.attention.update_temperature(temperature)<br><br>    def _forward_impl_common(self, x):<br><br>        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)<br>        batch_size, in_planes, height, width = x.size()<br>        x = x * channel_attention<br>        x = x.reshape(1, -1, height, width)<br>        aggregate_weight = spatial_attention * kernel_attention * self.weight.unsqueeze(dim=0)<br>        aggregate_weight = torch.sum(aggregate_weight, dim=1).view(<br>            [-1, self.in_planes // self.groups, self.kernel_size, self.kernel_size])<br>        output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,<br>                          dilation=self.dilation, groups=self.groups * batch_size)<br>        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))<br>        output = output * filter_attention<br>        return output<br><br>    def _forward_impl_pw1x(self, x):<br>        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)<br>        x = x * channel_attention<br>        output = F.conv2d(x, weight=self.weight.squeeze(dim=0), bias=None, stride=self.stride, padding=self.padding,<br>                          dilation=self.dilation, groups=self.groups)<br>        output = output * filter_attention<br>        return output<br><br>    def forward(self, x):<br>        return self._forward_impl(x)<br><br></code></pre></td></tr></table></figure>

<p><strong>2.在yolo.py文件中添加对应的CBAM</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">elif m in [ODConv]:<br>    c1, c2 = ch[f], args[0]<br>    if c2 != no:  # if not output<br>        c2 = make_divisible(c2 * gw, 8)<br><br>    args = [c1, c2, *args[1:]]<br></code></pre></td></tr></table></figure>

<p>在yolov5.yaml文件中则需要除了backbone网络中第一个卷积之外，其它卷积都可以替换为ODConv<br>例如替换head中一个卷积conv</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">head:<br>  [[-1, 1, Conv, [512, 1, 1]],<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 6], 1, Concat, [1]],  # cat backbone P4<br>   [-1, 3, C3, [512, False]],  # 13<br><br>   [-1, 1, Conv, [256, 1, 1]],<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 4], 1, Concat, [1]],  # cat backbone P3<br>   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)<br><br>   [-1, 1, Conv, [256, 3, 2]],<br>   [[-1, 14], 1, Concat, [1]],  # cat head P4<br>   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)<br><br>   [-1, 1, ODConv, [512, 3, 2]],<br>   [[-1, 10], 1, Concat, [1]],  # cat head P5<br>   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)<br><br>   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)<br>  ]<br></code></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://qzpzd.github.io/myblog">guzhongyue666</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://qzpzd.github.io/myblog/2023/10/12/yolov5_head_CBAM/">https://qzpzd.github.io/myblog/2023/10/12/yolov5_head_CBAM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://qzpzd.github.io/myblog" target="_blank">两个世界的我</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/myblog/tags/yolov5-head-CBAM-ODConv/">-yolov5 -head -CBAM -ODConv</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2023/10/12/wUIdgiNVbkvB6ap.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/myblog/2023/10/12/yolov5_loss/" title="(四)yolov5修改损失函数权重系数|提升小目标检测"><img class="cover" src="https://s2.loli.net/2023/10/12/hJNCxOtMn5ID7iY.png" onerror="onerror=null;src='/myblog/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">(四)yolov5修改损失函数权重系数|提升小目标检测</div></div></a></div><div class="next-post pull-right"><a href="/myblog/2023/10/11/yolov5_head_BiFFPN/" title="(二)yolov5检测头head魔改之BiFPN|优化小目标检测涨点"><img class="cover" src="https://s2.loli.net/2023/10/11/Mrgms1X9bNBwe5o.png" onerror="onerror=null;src='/myblog/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">(二)yolov5检测头head魔改之BiFPN|优化小目标检测涨点</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/myblog/img/favicon.jpg" onerror="this.onerror=null;this.src='/myblog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">guzhongyue666</div><div class="author-info__description">主要涉及机器学习、深度学习、目标检测、分割、激光雷达、人工智能、自动驾驶领域</div></div><div class="card-info-data site-data is-center"><a href="/myblog/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/myblog/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/myblog/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qzpzd"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/qzpzd" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:qzpzd1227@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://blog.csdn.net/m0_47709941?type=blog" target="_blank" title="Csdn"><i class="fas fa-copyright" style="color: #6495ED;"></i></a><a class="social-icon" href="https://www.cnblogs.com/qzpzd" target="_blank" title="Cnblogs"><i class="fas fa-blog" style="color: #800080;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-CBAM%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">1.CBAM介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-common-py%E4%B8%AD%E5%8A%A0%E5%85%A5CBAM%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">2.common.py中加入CBAM代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%9C%A8yolo-py%E6%96%87%E4%BB%B6%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%AF%B9%E5%BA%94%E7%9A%84CBAM"><span class="toc-number">3.</span> <span class="toc-text">3.在yolo.py文件中添加对应的CBAM</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%9C%A8yolov5s-yaml%E6%96%87%E4%BB%B6%E4%B8%AD%E6%B7%BB%E5%8A%A0CBAM"><span class="toc-number">4.</span> <span class="toc-text">4.在yolov5s.yaml文件中添加CBAM</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E6%B3%A8%E6%84%8F%E5%8A%9BSE%EF%BC%8CECA%E4%B8%8ECSBAM%E5%8C%BA%E5%88%AB"><span class="toc-number">5.</span> <span class="toc-text">其它注意力SE，ECA与CSBAM区别</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%A6%E4%B8%80%E5%88%A9%E7%94%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97%E7%9A%84ODConv%EF%BC%9A%E5%8D%B3%E6%8F%92%E5%8D%B3%E7%94%A8%E7%9A%84%E5%8A%A8%E6%80%81%E5%8D%B7%E7%A7%AF"><span class="toc-number">6.</span> <span class="toc-text">另一利用注意力模块的ODConv：即插即用的动态卷积</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E5%9C%A8yolov5%E4%B8%AD%E6%B7%BB%E5%8A%A0ODConv"><span class="toc-number">7.</span> <span class="toc-text">5.在yolov5中添加ODConv</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/myblog/2023/10/12/yolov5_loss/" title="(四)yolov5修改损失函数权重系数|提升小目标检测"><img src="https://s2.loli.net/2023/10/12/hJNCxOtMn5ID7iY.png" onerror="this.onerror=null;this.src='/myblog/img/404.jpg'" alt="(四)yolov5修改损失函数权重系数|提升小目标检测"/></a><div class="content"><a class="title" href="/myblog/2023/10/12/yolov5_loss/" title="(四)yolov5修改损失函数权重系数|提升小目标检测">(四)yolov5修改损失函数权重系数|提升小目标检测</a><time datetime="2023-10-12T06:51:35.000Z" title="发表于 2023-10-12 14:51:35">2023-10-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myblog/2023/10/12/yolov5_head_CBAM/" title="(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力"><img src="https://s2.loli.net/2023/10/12/wUIdgiNVbkvB6ap.png" onerror="this.onerror=null;this.src='/myblog/img/404.jpg'" alt="(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力"/></a><div class="content"><a class="title" href="/myblog/2023/10/12/yolov5_head_CBAM/" title="(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力">(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力</a><time datetime="2023-10-12T06:51:35.000Z" title="发表于 2023-10-12 14:51:35">2023-10-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myblog/2023/10/11/yolov5_head_BiFFPN/" title="(二)yolov5检测头head魔改之BiFPN|优化小目标检测涨点"><img src="https://s2.loli.net/2023/10/11/Mrgms1X9bNBwe5o.png" onerror="this.onerror=null;this.src='/myblog/img/404.jpg'" alt="(二)yolov5检测头head魔改之BiFPN|优化小目标检测涨点"/></a><div class="content"><a class="title" href="/myblog/2023/10/11/yolov5_head_BiFFPN/" title="(二)yolov5检测头head魔改之BiFPN|优化小目标检测涨点">(二)yolov5检测头head魔改之BiFPN|优化小目标检测涨点</a><time datetime="2023-10-11T10:12:35.000Z" title="发表于 2023-10-11 18:12:35">2023-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myblog/2023/10/11/yolov5_head/" title="(一)yolov5检测头head魔改之添加小目标层|针对小目标检测"><img src="https://s2.loli.net/2023/10/11/8rF7lbhcgaIORAk.png" onerror="this.onerror=null;this.src='/myblog/img/404.jpg'" alt="(一)yolov5检测头head魔改之添加小目标层|针对小目标检测"/></a><div class="content"><a class="title" href="/myblog/2023/10/11/yolov5_head/" title="(一)yolov5检测头head魔改之添加小目标层|针对小目标检测">(一)yolov5检测头head魔改之添加小目标层|针对小目标检测</a><time datetime="2023-10-11T06:51:35.000Z" title="发表于 2023-10-11 14:51:35">2023-10-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/myblog/2023/10/09/hello-world/" title="Hello World">Hello World</a><time datetime="2023-10-09T11:05:01.773Z" title="发表于 2023-10-09 19:05:01">2023-10-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By guzhongyue666</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">与君共勉</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/myblog/js/utils.js"></script><script src="/myblog/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '37Qomll1H0CF832dRWKZEDeh-gzGzoHsz',
      appKey: 'K4cMZGM3f3CllZ22vtRCTM52',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="/myblog/js/myjs.js"></script><div class="aplayer no-destroy" data-id="697054881" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-autoplay="true" > </div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/myblog/js/search/local-search.js"></script></div></div></body></html>