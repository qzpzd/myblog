<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>onnx模型和手动生成anchors文件进行后处理与测试</title>
    <url>/myblog/2023/08/10/onnx_detect/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/myblog/css/APlayer.min.css"><script src="/myblog/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/myblog/js/Meting.min.js"></script><p>训练好的深度学习模型通常需要转换为onnx模型，官方有onnx的测试代码，这里重新进行构建，并手动生成anchor文件，进行onnx模型测试，对于输入图像前处理也分为两部分，一个包含letterbox；一个不包含直接resize，显然后一个精度会有多降低</p>
<h1 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h1><figure class="highlight plaintext"><figcaption><span>[后处理]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">import onnxruntime<br>import cv2<br>import numpy as np<br>import torch<br>#---------------------NMS--------------------------------------------------------------------<br>def py_cpu_nms(dets0, conf_thresh, iou_thresh):<br>    &quot;&quot;&quot;Pure Python NMS baseline.&quot;&quot;&quot;<br>    nc = dets0.shape[1] - 5<br>    dets = dets0[dets0[:, 4] &gt; conf_thresh]<br>    dets = xywh2xyxy(dets)<br>    <br>    keep_all = []<br>    for cls in range(nc):<br>        dets_single = dets[np.argmax(dets[:,5:],axis=1)==cls]<br>        #print(&#x27;dets_single %d&#x27;%cls,dets_single)<br>        x1 = dets_single[:, 0]<br>        y1 = dets_single[:, 1]<br>        x2 = dets_single[:, 2]<br>        y2 = dets_single[:, 3]<br>        scores = dets_single[:, 4]<br>        areas = (x2 - x1 + 1) * (y2 - y1 + 1) <br>        order = scores.argsort()[::-1]<br>        keep = []<br>        while order.size &gt; 0:<br>            i = order[0]<br>            keep.append(i)  <br>            xx1 = np.maximum(x1[i], x1[order[1:]])<br>            yy1 = np.maximum(y1[i], y1[order[1:]])<br>            xx2 = np.minimum(x2[i], x2[order[1:]])<br>            yy2 = np.minimum(y2[i], y2[order[1:]])<br>            w = np.maximum(0.0, xx2 - xx1 + 1)<br>            h = np.maximum(0.0, yy2 - yy1 + 1)<br>            inter = w * h<br>            ovr = inter / (areas[i] + areas[order[1:]] - inter)<br>            inds = np.where(ovr &lt;= iou_thresh)[0]<br>            order = order[inds + 1]<br>        keep_rect = dets_single[keep]<br>        #print(&#x27;keep&#x27;,keep)<br>        keep_all.extend(keep_rect)<br>    return keep_all<br><br>def xywh2xyxy(x):<br>    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right<br>    y = np.zeros_like(x)<br>    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x<br>    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y<br>    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x<br>    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y<br>    y[:, 4:] = x[:,4:]<br>    return y<br><br>#---------------------img_preprocess-----------------------------------------------------------------<br>def img_preprocess(frame,imgsz):<br>    # im = letterbox(frame, imgsz)[0]<br>    im = cv2.resize(frame,(640,384))<br>    im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB<br>    im = np.ascontiguousarray(im)  # contiguous<br>    im = np.asarray(im, dtype=np.float32)<br>    im = np.expand_dims(im, 0)<br>    im /= 255.0<br>    return im<br><br>#---------------------------letterbox------------------------------------------------------------------<br>def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):<br>    # Resize and pad image while meeting stride-multiple constraints<br>    shape = im.shape[:2]  # current shape [height, width]<br>    if isinstance(new_shape, int):<br>        new_shape = (new_shape, new_shape)<br><br>    # Scale ratio (new / old)<br>    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])<br>    if not scaleup:  # only scale down, do not scale up (for better val mAP)<br>        r = min(r, 1.0)<br>    <br>    # Compute padding<br>    ratio = r, r  # width, height ratios<br>    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))<br>    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding<br>    if auto:  # minimum rectangle<br>        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding<br>    elif scaleFill:  # stretch<br>        dw, dh = 0.0, 0.0<br>        new_unpad = (new_shape[1], new_shape[0])<br>        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios<br>    <br>    dw /= 2  # divide padding into 2 sides<br>    dh /= 2<br>    <br>    if shape[::-1] != new_unpad:  # resize<br>        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)<br>    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))<br>    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))<br>    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border<br>    return im, ratio, (dw, dh)<br><br>#-----------------------------decode----------------------------------------<br>def np_sigmoid(x):<br>    return 1.0/(1.0+1.0/np.exp(x))<br>    <br>def decode_output(pred_raw_data,anchor_txt):<br>    pred_raw_data = np_sigmoid(pred_raw_data)<br>    print(max(pred_raw_data[:, 4]))<br>    pred_raw_data[:, 0] = (pred_raw_data[:, 0] * 2. - 0.5 + anchor_txt[:, 0]) * anchor_txt[:, 4] #x<br>    pred_raw_data[:, 1] = (pred_raw_data[:, 1] * 2. - 0.5 + anchor_txt[:, 1]) * anchor_txt[:, 4] #y<br>    pred_raw_data[:, 2] = (pred_raw_data[:, 2] * 2) ** 2 * anchor_txt[:, 2]  # w<br>    pred_raw_data[:, 3] = (pred_raw_data[:, 3] * 2) ** 2 * anchor_txt[:, 3]  # h<br>    <br>    return pred_raw_data<br><br>#-------------------------scale_ratio------------------------------------------------------<br>def helmet_scale_ratio(each,frame,imgsz=640):<br>    ratio = (frame.shape[0] /384 , frame.shape[1] / imgsz)<br>    each[[0, 2]] *= ratio[1]<br>    each[[1, 3]] *= ratio[0]<br>    return each<br>     <br>#---------------------helmet_detect----------------------------------------------------------------------------<br>def helmet_detect(face_model,frame):<br>    anchors = np.fromfile(&#x27;C:/Users/suso/Desktop/yolov5_fire/yolov5_fire_priorbox_384-640.txt&#x27;,sep=&#x27; &#x27;)<br>    anchors = anchors.reshape(-1,5)<br>    imgsz = 640<br>    img = img_preprocess(frame,imgsz) <br><br>    session = onnxruntime.InferenceSession(face_model)<br>    in_name = [input.name for input in session.get_inputs()][0]<br>    out_name = [output.name for output in session.get_outputs()]              <br>    pred = session.run(out_name,&#123;in_name: img&#125;)  <br><br><br>​    <br>    x1 = np.array(pred[0]).reshape(-1, 6)<br>    x2 = np.array(pred[1]).reshape(-1, 6)<br>    x3 = np.array(pred[2]).reshape(-1, 6)<br>    print(x3.shape,max(x3[:,4]))<br>    <br>    try:<br>        # Save x1 to file<br>        np.savetxt(&quot;C:/Users\suso\Desktop/yolov5_fire/x1.txt&quot;, x1, delimiter=&#x27; &#x27;, fmt=&#x27;%f&#x27;)<br>        # Save x2 to file<br>        np.savetxt(&quot;C:/Users\suso\Desktop/yolov5_fire/x2.txt&quot;, x2, delimiter=&#x27; &#x27;, fmt=&#x27;%f&#x27;)<br>        # Save x3 to file<br>        np.savetxt(&quot;C:/Users\suso\Desktop/yolov5_fire/x3.txt&quot;, x3, delimiter=&#x27; &#x27;, fmt=&#x27;%f&#x27;)<br>    except Exception as e:<br>        print(f&quot;An error occurred: &#123;str(e)&#125;&quot;)<br>    <br>    out_data_raw = np.vstack((x1,x2,x3))<br>    np.savetxt(&quot;C:/Users\suso\Desktop/yolov5_fire/x4.txt&quot;, out_data_raw, delimiter=&#x27; &#x27;, fmt=&#x27;%f&#x27;)<br>    output_from_txt = decode_output(out_data_raw,anchors)<br>    <br>    print(len(pred),pred[0].shape)<br>    print(&quot;ffffffffff&quot;,max(np.array(pred[0]).reshape(-1, 6)[:,4]))<br>    # pred = py_cpu_nms(np.array(pred[0]).reshape(-1, 6), 0.2, 0.45)<br>    pred = py_cpu_nms(output_from_txt, 0.2, 0.45)<br>    <br>    return pred,img<br><br>#---------------draw--------------------------------------------<br>def drawHelmetBox(frame,bbox):<br>    print(bbox)<br>    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 3)<br>    label = f&#x27;&#123;float(bbox[4]*bbox[5]):.2f&#125;&#x27;<br>    cv2.putText(frame, label, (int(bbox[0]), int(bbox[1])+20), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)<br>    cv2.imshow(&#x27;frame&#x27;,frame)<br>    cv2.waitKey(0)<br>    cv2.destroyAllWindows()<br><br>def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):<br>    # Rescale boxes (xyxy) from img1_shape to img0_shape<br>    if ratio_pad is None:  # calculate from img0_shape<br>        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new<br>        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding<br>    else:<br>        gain = ratio_pad[0][0]<br>        pad = ratio_pad[1]<br><br>    boxes[..., [0, 2]] -= pad[0]  # x padding<br>    boxes[..., [1, 3]] -= pad[1]  # y padding<br>    boxes[..., :4] /= gain<br>    clip_boxes(boxes, img0_shape)<br>    return boxes<br><br>def clip_boxes(boxes, shape):<br>    # Clip boxes (xyxy) to image shape (height, width)<br>    if isinstance(boxes, torch.Tensor):  # faster individually<br>        boxes[..., 0].clamp_(0, shape[1])  # x1<br>        boxes[..., 1].clamp_(0, shape[0])  # y1<br>        boxes[..., 2].clamp_(0, shape[1])  # x2<br>        boxes[..., 3].clamp_(0, shape[0])  # y2<br>    else:  # np.array (faster grouped)<br>        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2<br>        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2<br></code></pre></td></tr></table></figure>

<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><figure class="highlight plaintext"><figcaption><span>[测试]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">import cv2<br>import os<br>from HelmetDetection import helmet_detect,helmet_scale_ratio,drawHelmetBox,scale_boxes<br><br># image_path = &#x27;C:/Users/suso/Desktop/yolov5_fire/frame_01_jpg.rf.7a9e0fe6e03efe5b3df01c7322aff0dc.jpg&#x27;<br>face_model = &#x27;C:/Users/suso/Desktop/yolov5_fire/best.onnx&#x27;<br>for imgname in os.listdir(&#x27;C:/Users/suso/Desktop/yolov5_fire/test_jpg&#x27;):<br>    print(imgname)<br>    image_path = &#x27;C:/Users/suso/Desktop/yolov5_fire/test_jpg/&#x27;+imgname<br>    frame = cv2.imread(image_path)<br>    helmet_pred,img = helmet_detect(face_model,frame)<br>    print(helmet_pred)<br>    for bbox in helmet_pred:<br>        bbox = helmet_scale_ratio(bbox,frame)#no letterbox<br>        # im_shape = img.shape[2:]<br>        # frame_shape = frame.shape<br>        # bbox = scale_boxes(im_shape, bbox, frame_shape)#add letterbox<br>        drawHelmetBox(frame,bbox)<br><br><br></code></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>-html -onnx -detect</tag>
      </tags>
  </entry>
  <entry>
    <title>(一)yolov5检测头head魔改之添加小目标层|针对小目标检测</title>
    <url>/myblog/2023/10/11/yolov5_head/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/myblog/css/APlayer.min.css"><script src="/myblog/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/myblog/js/Meting.min.js"></script><p><strong>1.YOLOv5算法简介</strong></p>
<p>YOLOv5主要由输入端、Backone、Neck以及Prediction四部分组成。其中：</p>
<p>(1) Backbone：在不同图像细粒度上聚合并形成图像特征的卷积神经网络。</p>
<p>(2) Neck：一系列混合和组合图像特征的网络层，并将图像特征传递到预测层。</p>
<p>(3) Head： 对图像特征进行预测，生成边界框和并预测类别。</p>
<p>检测框架：</p>
<p><img src="https://s2.loli.net/2023/10/11/8rF7lbhcgaIORAk.png" alt="image.png"></p>
<p><strong>2.原始YOLOv5模型</strong></p>
<figure class="highlight plaintext"><figcaption><span>[原始]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"># YOLOv5 head<br>head:<br>  [[-1, 1, Conv, [512, 1, 1]],<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 6], 1, Concat, [1]],  # cat backbone P4<br>   [-1, 3, C3, [512, False]],  # 13<br><br>   [-1, 1, Conv, [256, 1, 1]],<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 4], 1, Concat, [1]],  # cat backbone P3<br>   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)<br><br>   [-1, 1, Conv, [256, 3, 2]],<br>   [[-1, 14], 1, Concat, [1]],  # cat head P4<br>   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)<br><br>   [-1, 1, Conv, [512, 3, 2]],<br>   [[-1, 10], 1, Concat, [1]],  # cat head P5<br>   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)<br><br>   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)<br>  ]<br><br><br></code></pre></td></tr></table></figure>

<p>若输入图像尺寸&#x3D;640X640，</p>
<p>P3&#x2F;8 对应的检测特征图大小为80X80，用于检测大小在8X8以上的目标。</p>
<p>P4&#x2F;16对应的检测特征图大小为40X40，用于检测大小在16X16以上的目标。</p>
<p>P5&#x2F;32对应的检测特征图大小为20X20，用于检测大小在32X32以上的目标。</p>
<p><strong>3.增加小目标检测层</strong></p>
<figure class="highlight plaintext"><figcaption><span>[魔改]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"><br># parameters<br>nc: 1  # number of classes<br>depth_multiple: 0.33  # model depth multiple<br>width_multiple: 0.50  # layer channel multiple<br><br># anchors<br>anchors:<br>  - [5,6, 8,14, 15,11]  #4<br>  - [10,13, 16,30, 33,23]  # P3/8<br>  - [30,61, 62,45, 59,119]  # P4/16<br>  - [116,90, 156,198, 373,326]  # P5/32<br><br># YOLOv5 backbone<br>backbone:<br>  # [from, number, module, args]<br>  [[-1, 1, Focus, [64, 3]],  # 0-P1/2<br>   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4<br>   [-1, 3, BottleneckCSP, [128]],   #160*160<br>   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8<br>   [-1, 9, BottleneckCSP, [256]],  #80*80<br>   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16<br>   [-1, 9, BottleneckCSP, [512]], #40*40<br>   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32<br>   [-1, 1, SPP, [1024, [5, 9, 13]]],<br>   [-1, 3, BottleneckCSP, [1024, False]],  # 9   20*20<br>  ]<br><br># YOLOv5 head<br>head:<br>  [[-1, 1, Conv, [512, 1, 1]],  #20*20<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]], #40*40<br>   [[-1, 6], 1, Concat, [1]],  # cat backbone P4  40*40<br>   [-1, 3, BottleneckCSP, [512, False]],  # 13     40*40<br><br>   [-1, 1, Conv, [512, 1, 1]], #40*40<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 4], 1, Concat, [1]],  # cat backbone P3   80*80<br>   [-1, 3, BottleneckCSP, [512, False]],  # 17 (P3/8-small)  80*80<br><br>   [-1, 1, Conv, [256, 1, 1]], #18  80*80<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]], #19  160*160<br>   [[-1, 2], 1, Concat, [1]], #20 cat backbone p2  160*160<br>   [-1, 3, BottleneckCSP, [256, False]], #21 160*160<br><br>   [-1, 1, Conv, [256, 3, 2]],  #22   80*80<br>   [[-1, 18], 1, Concat, [1]], #23 80*80<br>   [-1, 3, BottleneckCSP, [256, False]], #24 80*80<br><br>   [-1, 1, Conv, [256, 3, 2]], #25  40*40<br>   [[-1, 14], 1, Concat, [1]],  # 26  cat head P4  40*40<br>   [-1, 3, BottleneckCSP, [512, False]],  # 27 (P4/16-medium) 40*40<br><br>   [-1, 1, Conv, [512, 3, 2]],  #28  20*20<br>   [[-1, 10], 1, Concat, [1]],  #29 cat head P5  #20*20<br>   [-1, 3, BottleneckCSP, [1024, False]],  # 30 (P5/32-large)  20*20<br><br>   [[21, 24, 27, 30], 1, Detect, [nc, anchors]],  # Detect(p2, P3, P4, P5)<br>  ]<br><br></code></pre></td></tr></table></figure>

<p>上面的代码中根据不同yolov5版本进行适当修改，例如有的版本backbone第一行为[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1&#x2F;2，利用卷积取消了Focus，有些版本是C3替换了C2f，backbone第一行为[-1, 1, Conv, [64, 3, 2]]  # 0-P1&#x2F;2，其它yolo系列如yolov7与yolov8原理一样，之不过yolov5与yolov7许哟啊用到anchors，因此anchor也需要在原来基础上增加一个如上，yolov8则不需要，然后都是根据对应版本的原始yaml文件进行更改，只更改head部分即可。</p>
<p><strong># 新增加160X160的检测特征图，用于检测4X4以上的目标。</strong></p>
<p>改进后，虽然计算量和检测速度有所增加，但对小目标的检测精度有明显改善。</p>
<p>这里给出其它yolov7与yolov8魔改的链接</p>
<p><a href="https://blog.csdn.net/m0_70388905/article/details/125392908">YOLOv8&#x2F;YOLOv7&#x2F;YOLOv5系列算法改进【NO.6】增加小目标检测层，提高对小目标的检测效果_yolov5添加小目标检测层_人工智能算法研究院的博客-CSDN博客</a></p>
<p>anchor重新聚类方法<br><a href="https://blog.csdn.net/qq_37541097/article/details/119647026">https://blog.csdn.net/qq_37541097/article/details/119647026</a></p>
<p>yolov5中聚类anchors代码讲解<br>如果你是直接使用yolov5的训练脚本，那么它会自动去计算下默认的anchors与你数据集中所有目标的best possible recall，如果小于0.98就会根据你自己数据集的目标去重新聚类生成anchors，反之使用默认的anchors。</p>
<p>下面代码是我根据yolov5中聚类anchors的代码简单修改得到的。基本流程不变，主要改动了三点：1.对代码做了些简化。2.把使用pytorch的地方都改成了numpy（感觉这样会更通用点，但numpy效率确实没有pytorch高）。3.作者默认使用的k-means方法是scipy包提供的，使用的是欧式距离。我自己改成了基于1-IOU(bboxes, anchors)距离的方法。当然我只是注释掉了作者原来的方法，如果想用自己把注释取消掉就行了。但在我使用测试过程中，还是基于1-IOU(bboxes, anchors)距离的方法会略好点。</p>
<p>完整代码链接：<br><a href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/others_project/kmeans_anchors">https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/others_project/kmeans_anchors</a></p>
<p>其实在yolov5生成anchors中不仅仅使用了k-means聚类，还使用了Genetic Algorithm遗传算法，在k-means聚类的结果上进行mutation变异。接下来简单介绍下代码流程：</p>
<p>读取训练集中每张图片的wh以及所有bboxes的wh（这里是我自己写的脚本读取的PASCAL VOC数据）<br>将每张图片中wh的最大值等比例缩放到指定大小img_size，由于读取的bboxes是相对坐标所以不需要改动<br>将bboxes从相对坐标改成绝对坐标（乘以缩放后的wh）<br>筛选bboxes，保留wh都大于等于两个像素的bboxes<br>使用k-means聚类得到n个anchors<br>使用遗传算法随机对anchors的wh进行变异，如果变异后效果变得更好（使用anchor_fitness方法计算得到的fitness（适应度）进行评估）就将变异后的结果赋值给anchors，如果变异后效果变差就跳过，默认变异1000次。<br>将最终变异得到的anchors按照面积进行排序并返回</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">import random<br>import numpy as np<br>from tqdm import tqdm<br>from scipy.cluster.vq import kmeans<br><br>from read_voc import VOCDataSet<br>from yolo_kmeans import k_means, wh_iou<br><br><br>def anchor_fitness(k: np.ndarray, wh: np.ndarray, thr: float):  # mutation fitness<br>    r = wh[:, None] / k[None]<br>    x = np.minimum(r, 1. / r).min(2)  # ratio metric<br>    # x = wh_iou(wh, k)  # iou metric<br>    best = x.max(1)<br>    f = (best * (best &gt; thr).astype(np.float32)).mean()  # fitness<br>    bpr = (best &gt; thr).astype(np.float32).mean()  # best possible recall<br>    return f, bpr<br><br><br>def main(img_size=512, n=9, thr=0.25, gen=1000):<br>    # 从数据集中读取所有图片的wh以及对应bboxes的wh<br>    dataset = VOCDataSet(voc_root=&quot;/data&quot;, year=&quot;2012&quot;, txt_name=&quot;train.txt&quot;)<br>    im_wh, boxes_wh = dataset.get_info()<br><br>    # 最大边缩放到img_size<br>    im_wh = np.array(im_wh, dtype=np.float32)<br>    shapes = img_size * im_wh / im_wh.max(1, keepdims=True)<br>    wh0 = np.concatenate([l * s for s, l in zip(shapes, boxes_wh)])  # wh<br><br>    # Filter 过滤掉小目标<br>    i = (wh0 &lt; 3.0).any(1).sum()<br>    if i:<br>        print(f&#x27;WARNING: Extremely small objects found. &#123;i&#125; of &#123;len(wh0)&#125; labels are &lt; 3 pixels in size.&#x27;)<br>    wh = wh0[(wh0 &gt;= 2.0).any(1)]  # 只保留wh都大于等于2个像素的box<br><br>    # Kmeans calculation<br>    # print(f&#x27;Running kmeans for &#123;n&#125; anchors on &#123;len(wh)&#125; points...&#x27;)<br>    # s = wh.std(0)  # sigmas for whitening<br>    # k, dist = kmeans(wh / s, n, iter=30)  # points, mean distance<br>    # assert len(k) == n, print(f&#x27;ERROR: scipy.cluster.vq.kmeans requested &#123;n&#125; points but returned only &#123;len(k)&#125;&#x27;)<br>    # k *= s<br>    k = k_means(wh, n)<br><br>    # 按面积排序<br>    k = k[np.argsort(k.prod(1))]  # sort small to large<br>    f, bpr = anchor_fitness(k, wh, thr)<br>    print(&quot;kmeans: &quot; + &quot; &quot;.join([f&quot;[&#123;int(i[0])&#125;, &#123;int(i[1])&#125;]&quot; for i in k]))<br>    print(f&quot;fitness: &#123;f:.5f&#125;, best possible recall: &#123;bpr:.5f&#125;&quot;)<br><br>    # Evolve<br>    # 遗传算法(在kmeans的结果基础上变异mutation)<br>    npr = np.random<br>    f, sh, mp, s = anchor_fitness(k, wh, thr)[0], k.shape, 0.9, 0.1  # fitness, generations, mutation prob, sigma<br>    pbar = tqdm(range(gen), desc=f&#x27;Evolving anchors with Genetic Algorithm:&#x27;)  # progress bar<br>    for _ in pbar:<br>        v = np.ones(sh)<br>        while (v == 1).all():  # mutate until a change occurs (prevent duplicates)<br>            v = ((npr.random(sh) &lt; mp) * random.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)<br>        kg = (k.copy() * v).clip(min=2.0)<br>        fg, bpr = anchor_fitness(kg, wh, thr)<br>        if fg &gt; f:<br>            f, k = fg, kg.copy()<br>            pbar.desc = f&#x27;Evolving anchors with Genetic Algorithm: fitness = &#123;f:.4f&#125;&#x27;<br><br>    # 按面积排序<br>    k = k[np.argsort(k.prod(1))]  # sort small to large<br>    print(&quot;genetic: &quot; + &quot; &quot;.join([f&quot;[&#123;int(i[0])&#125;, &#123;int(i[1])&#125;]&quot; for i in k]))<br>    print(f&quot;fitness: &#123;f:.5f&#125;, best possible recall: &#123;bpr:.5f&#125;&quot;)<br><br><br>if __name__ == &quot;__main__&quot;:<br>    main()<br><br></code></pre></td></tr></table></figure>


<h1 id="聚类anchors需要注意的坑"><a href="#聚类anchors需要注意的坑" class="headerlink" title="聚类anchors需要注意的坑"></a>聚类anchors需要注意的坑</h1><p>有时使用自己聚类得到的anchors的效果反而变差了，此时你可以从以下几方面进行检查：</p>
<p>注意输入网络时训练的图片尺寸。这是个很重要的点，因为一般训练&#x2F;验证时输入网络的图片尺寸是固定的，比如说640x640，那么图片在输入网络前一般会将最大边长缩放到640，同时图片中的bboxes也会进行缩放。所以在聚类anchors时需要使用相同的方式提前去缩放bboxes，否则聚类出来的anchors并不匹配。比如你的图片都是1280x1280大小的，假设bboxes都是100x100大小的，如果不去缩放bboxes，那么聚类得到的anchors差不多是在100x100附近。而实际训练网络时bboxes都已经缩放到50x50大小了，此时理想的anchors应该是50x50左右而不是100x100了。<br>如果使用预训练权重，不要冻结太多的权重。现在训练自己数据集时一般都是使用别人在coco等大型数据上预训练好的权重。而这些权重是基于coco等数据集上聚类得到的结果，并不是针对自己数据集聚类得到的。所以网络为了要适应新的anchors需要调整很多权重，如果你冻结了很多层（假设只去微调最后的预测器，其他权重全部冻结），那么得到的结果很大几率还没有之前的anchors好。当可训练的权重越来越多，一般使用自己数据集聚类得到的anchors会更好一点（前提是自己聚类的anchors是合理的）。</p>
]]></content>
      <tags>
        <tag>-yolov5 -head</tag>
      </tags>
  </entry>
  <entry>
    <title>(三)yolov5修改损失函数权重系数|提升小目标检测</title>
    <url>/myblog/2023/10/12/yolov5_loss/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/myblog/css/APlayer.min.css"><script src="/myblog/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/myblog/js/Meting.min.js"></script><h1 id="损失计算"><a href="#损失计算" class="headerlink" title="损失计算"></a>损失计算</h1><p>YOLOv5的损失主要由三个部分组成：</p>
<p>Classes loss，分类损失，采用的是BCE loss，注意只计算正样本的分类损失。 </p>
<p>Objectness loss，obj损失，采用的依然是BCE loss，注意这里的obj指的是网络预测的目标边界框与GT Box的CIoU。这里计算的是所有样本的obj损失。 </p>
<p>Location loss，定位损失，采用的是CIoU loss，注意只计算正样本的定位损失。<br><img src="https://s2.loli.net/2023/10/12/my9WaDVM81bsNCF.png" alt="image.png"></p>
<p>在源码中，针对预测小目标的预测特征层（P3）采用的权重是4.0，针对预测中等目标的预测特征层（P4）采用的权重是1.0，针对预测大目标的预测特征层（P5）采用的权重是0.4，作者说这是针对COCO数据集设置的超参数。</p>
<p><img src="https://s2.loli.net/2023/10/12/1q4WwCNskn8SEVl.png" alt="image.png"></p>
<p>对应在loss.py文件中的class ComputeLoss类找到下面代码，修改相应的数值</p>
<figure class="highlight plaintext"><figcaption><span>[loss]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">self.balance = &#123;3: [4.0, 1.0, 0.4]&#125;.get(m.nl, [4.0, 1.0, 0.25, 0.06, 0.02])  # P3-P7<br></code></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>-yolov5 -loss</tag>
      </tags>
  </entry>
  <entry>
    <title>butterfly教程</title>
    <url>/myblog/2023/07/27/test/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/myblog/css/APlayer.min.css"><script src="/myblog/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/myblog/js/Meting.min.js"></script><h1 id="引用块"><a href="#引用块" class="headerlink" title="引用块"></a>引用块</h1><p>源代码</p>
<figure class="highlight plaintext"><figcaption><span>[引用块]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">&lt;blockquote&gt;&lt;p&gt;content  &lt;/p&gt;<br>&lt;footer&gt;&lt;strong&gt;author&lt;/strong&gt;&lt;cite&gt;- source [link] [source_link_title]&lt;/cite&gt;&lt;/footer&gt;&lt;/blockquote&gt;<br></code></pre></td></tr></table></figure>

<blockquote><p>content  </p>
<footer><strong>author</strong><cite>- source [link] [source_link_title]</cite></footer></blockquote>

<p>源代码</p>
<figure class="highlight plaintext"><figcaption><span>[引用块]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"><br>&gt;引用内容<br>&gt;&gt;引用内容<br>&gt;&gt;&gt;yy内容<br><br></code></pre></td></tr></table></figure>

<blockquote>
<p>引用内容</p>
<blockquote>
<p>引用内容</p>
<blockquote>
<p>yy内容</p>
</blockquote>
</blockquote>
</blockquote>
<h1 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h1><p>源代码</p>
<figure class="highlight plaintext"><figcaption><span>[分割线]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"><br>---<br><br></code></pre></td></tr></table></figure>

<hr>
<h1 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h1><p>源代码</p>
<figure class="highlight plaintext"><figcaption><span>[代码块]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"><br>&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;figcaption&gt;&lt;span&gt;[title] [lang:language] [url] [link text]&lt;/span&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;code class=&quot;hljs plaintext&quot;&gt;  &lt;br&gt;code snippet  &lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;  <br><br></code></pre></td></tr></table></figure>

<figure class="highlight plaintext"><figcaption><span>[title] [lang:language] [url] [link text]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">  <br>code snippet  <br></code></pre></td></tr></table></figure>  

<h1 id="引用文章链接"><a href="#引用文章链接" class="headerlink" title="引用文章链接"></a>引用文章链接</h1><p>源代码</p>
<figure class="highlight plaintext"><figcaption><span>[链接块]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"><br>&lt;a href=&quot;/2023/10/09/hello-world/&quot; title=&quot;Hello World&quot;&gt;hello-world&lt;/a&gt; <br><br></code></pre></td></tr></table></figure>

<a href="/2023/10/09/hello-world/" title="Hello World">hello-world</a>

<h1 id="选项卡"><a href="#选项卡" class="headerlink" title="选项卡"></a>选项卡</h1><p>源代码</p>
<figure class="highlight plaintext"><figcaption><span>[选项卡]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"><br>&lt;div class=&quot;tabs&quot; id=&quot;标签&quot;&gt;&lt;ul class=&quot;nav-tabs&quot;&gt;&lt;li class=&quot;tab active&quot;&gt;&lt;button type=&quot;button&quot; data-href=&quot;#标签-1&quot;&gt;标签 1&lt;/button&gt;&lt;/li&gt;&lt;li class=&quot;tab&quot;&gt;&lt;button type=&quot;button&quot; data-href=&quot;#标签-2&quot;&gt;标签 2&lt;/button&gt;&lt;/li&gt;&lt;li class=&quot;tab&quot;&gt;&lt;button type=&quot;button&quot; data-href=&quot;#标签-3&quot;&gt;标签三&lt;/button&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;tab-contents&quot;&gt;&lt;div class=&quot;tab-item-content active&quot; id=&quot;标签-1&quot;&gt;&lt;p&gt;&lt;strong&gt;选项卡 1&lt;/strong&gt; &lt;/p&gt;&lt;button type=&quot;button&quot; class=&quot;tab-to-top&quot; aria-label=&quot;scroll to top&quot;&gt;&lt;i class=&quot;fas fa-arrow-up&quot;&gt;&lt;/i&gt;&lt;/button&gt;&lt;/div&gt;&lt;div class=&quot;tab-item-content&quot; id=&quot;标签-2&quot;&gt;&lt;p&gt;&lt;strong&gt;选项卡 2&lt;/strong&gt;&lt;/p&gt;&lt;button type=&quot;button&quot; class=&quot;tab-to-top&quot; aria-label=&quot;scroll to top&quot;&gt;&lt;i class=&quot;fas fa-arrow-up&quot;&gt;&lt;/i&gt;&lt;/button&gt;&lt;/div&gt;&lt;div class=&quot;tab-item-content&quot; id=&quot;标签-3&quot;&gt;&lt;p&gt;&lt;strong&gt;选项卡 3&lt;/strong&gt; , 名字为 &lt;code&gt;TAB三&lt;/code&gt;&lt;/p&gt;&lt;button type=&quot;button&quot; class=&quot;tab-to-top&quot; aria-label=&quot;scroll to top&quot;&gt;&lt;i class=&quot;fas fa-arrow-up&quot;&gt;&lt;/i&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; <br><br></code></pre></td></tr></table></figure>


<div class="tabs" id="标签"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#标签-1">标签 1</button></li><li class="tab"><button type="button" data-href="#标签-2">标签 2</button></li><li class="tab"><button type="button" data-href="#标签-3">标签三</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="标签-1"><p><strong>选项卡 1</strong> </p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="标签-2"><p><strong>选项卡 2</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="标签-3"><p><strong>选项卡 3</strong> , 名字为 <code>TAB三</code></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>


<p>源代码</p>
<figure class="highlight plaintext"><figcaption><span>[选项卡]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"><br>&lt;div class=&quot;gallery-group-main&quot;&gt;<br><br>  &lt;figure class=&quot;gallery-group&quot;&gt;<br>  &lt;img class=&quot;gallery-group-img no-lightbox&quot; src=&#x27;https://api.aqcoder.cntoday&#x27; alt=&quot;Group Image Gallery&quot;&gt;<br>  &lt;figcaption&gt;<br>  &lt;div class=&quot;gallery-group-name&quot;&gt;壁纸&lt;/div&gt;<br>  &lt;p&gt;收藏的一些壁纸&lt;/p&gt;<br>  &lt;a href=&#x27;/myblog/Gallery/wallpaper/&#x27;&gt;&lt;/a&gt;<br>  &lt;/figcaption&gt;<br>  &lt;/figure&gt;<br>  <br>&lt;/div&gt;<br><br></code></pre></td></tr></table></figure>

<div class="gallery-group-main">

  <figure class="gallery-group">
  <img class="gallery-group-img no-lightbox" src='https://api.aqcoder.cntoday' alt="Group Image Gallery">
  <figcaption>
  <div class="gallery-group-name">壁纸</div>
  <p>收藏的一些壁纸</p>
  <a href='/myblog/gallery/wallpaper/'></a>
  </figcaption>
  </figure>
  
</div>


<h1 id="提示块"><a href="#提示块" class="headerlink" title="提示块"></a>提示块</h1><p>源代码</p>
<figure class="highlight plaintext"><figcaption><span>[提示块]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"><br>&lt;div class=&quot;note default simple&quot;&gt;&lt;p&gt;default 提示块&lt;/p&gt;<br>&lt;/div&gt;<br><br>&lt;div class=&quot;note primary simple&quot;&gt;&lt;p&gt;primary 提示块&lt;/p&gt;<br>&lt;/div&gt;<br><br>&lt;div class=&quot;note success simple&quot;&gt;&lt;p&gt;success 提示块&lt;/p&gt;<br>&lt;/div&gt;<br><br>&lt;div class=&quot;note info simple&quot;&gt;&lt;p&gt;info 提示块&lt;/p&gt;<br>&lt;/div&gt;<br><br>&lt;div class=&quot;note warning simple&quot;&gt;&lt;p&gt;warning 提示块&lt;/p&gt;<br>&lt;/div&gt;<br><br>&lt;div class=&quot;note danger simple&quot;&gt;&lt;p&gt;danger 提示块&lt;/p&gt;<br>&lt;/div&gt;<br><br></code></pre></td></tr></table></figure>


<div class="note default simple"><p>default 提示块</p>
</div>

<div class="note primary simple"><p>primary 提示块</p>
</div>

<div class="note success simple"><p>success 提示块</p>
</div>

<div class="note info simple"><p>info 提示块</p>
</div>

<div class="note warning simple"><p>warning 提示块</p>
</div>

<div class="note danger simple"><p>danger 提示块</p>
</div>







]]></content>
      <tags>
        <tag>-butterfly -教程</tag>
      </tags>
  </entry>
  <entry>
    <title>(二)yolov5检测头head魔改之BiFPN|优化小目标检测涨点</title>
    <url>/myblog/2023/10/11/yolov5_head_BiFFPN/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/myblog/css/APlayer.min.css"><script src="/myblog/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/myblog/js/Meting.min.js"></script><p><strong>1.YOLOv5算法原始head结构</strong></p>
<p>YOLOv5模型的Neck部分使用的是FPN+PAN结构，FPN是针对多尺度问题提出的，FPN结构是自上而下并且横向连接的，它利用金字塔的形式对尺度不同的特征图进行连接，将高层特征和低层特征进行融合。FPN与PAN结合，对来自不同骨干层的不同检测层进行参数聚合。这种组合虽然有效提高了网络的特征融合能力，但也会导致一个问题，即PAN结构的输入全部是FPN结构处理的特征信息，而骨干特征提取网络部分的原始特征信息存在一部分丢失。缺乏参与学习的原始信息很容易导致训练学习的偏差，影响检测的准确性。</p>
<p>从neck特征融合入手，引入加权双向特征金字塔BiFPN来加强特征图的底层信息，使不同尺度的特征图进行信息融合，从而加强特征信息。</p>
<p><strong>2.BiFPN原理与结构</strong><br>BiFPN是一种改进版的FPN网络结构，主要用于目标检测任务。该结构是加权且双向连接的，即自顶向下和自底向上结构，通过构造双向通道实现跨尺度连接，将特征提取网络中的特征直接与自下而上路径中的相对大小特征融合，保留了更浅的语义信息，而不会丢失太多的深层语义信息。</p>
<p>传统的特征融合时将尺度不同的特征图以相同权重进行加权，但是当输入的特征图分辨率不同时，以相同的权重进行加权对输出的特征图不平等。所以BiFPN根据不同输入特征的重要性设置不同的权重，同时反复采用这种结构来加强特征融合。</p>
<p>BiFPN结构中的加权融合方式采用快速归一化融合(Fast normalized fusion)，该融合方式是针对训练速度慢提出的，将权重放缩至0~1范围内，因没有使用Softmax方式，所以训练速度很快。跨尺度连接通过添加一个跳跃连接和双向路径来实现，自此实现了加权融合和双向跨尺度连接。</p>
<p>传统的特征融合往往只是简单的 feature map 叠加&#x2F;相加 (sum them up)，比如使用concat或者shortcut连接，而不对同时加进来的 feature map 进行区分。然而，不同的输入 feature map 具有不同的分辨率，它们对融合输入 feature map 的贡献也是不同的，因此简单的对他们进行相加或叠加处理并不是最佳的操作。所以这里我们提出了一种简单而高效的加权特融合的机制。<br>常见的带权特征融合有三种方法：<br><img src="https://s2.loli.net/2023/10/11/EdCqgTs3mtwOQyL.png" alt="image.png"></p>
<p>BiFPN结构如图所示<br><img src="https://s2.loli.net/2023/10/11/Mrgms1X9bNBwe5o.png" alt="image.png"></p>
<p>详细结构图如下：<br><img src="https://s2.loli.net/2023/10/11/7D2ZQEytdUqO4Ga.png" alt="image.png"><br>从这张图可以看出图中有三个分支与两个分支融合，对应下面的代码部分</p>
<p>与只有一个自顶向下和一个自底向上路径的PANet不同，我们处理每个双向路径(自顶向下和自底而上)路径作为一个特征网络层，并重复同一层多次，以实现更高层次的特征融合。如下图EfficientNet 的网络结构所示，我们对BiFPN是重复使用多次的。而这个使用次数也不是我们认为设定的，而是作为参数一起加入网络的设计当中，使用NAS技术算出来的。<br><img src="https://s2.loli.net/2023/10/11/QzS8NCevsHAVbiO.png" alt="image.png"></p>
<p>根据上面可以看出，BiFPN其实就是融合了FPN与PAN并增加了跳跃连接来获取浅层信息，所以对于小目标检测效果比较好。</p>
<p><strong>3.代码与文件修改</strong></p>
<h1 id="在Common-py中添加定义模块-Concat"><a href="#在Common-py中添加定义模块-Concat" class="headerlink" title="在Common.py中添加定义模块(Concat)"></a>在Common.py中添加定义模块(Concat)</h1><figure class="highlight plaintext"><figcaption><span>[BiFPN]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"># 结合BiFPN 设置可学习参数 学习不同分支的权重<br># 两个分支concat操作<br>class BiFPN_Concat2(nn.Module):<br>    def __init__(self, dimension=1):<br>        super(BiFPN_Concat2, self).__init__()<br>        self.d = dimension<br>        self.w = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)<br>        self.epsilon = 0.0001<br>        # 设置可学习参数 nn.Parameter的作用是：将一个不可训练的类型Tensor转换成可以训练的类型 <br>        parameter<br>        # 并且会向宿主模型注册该参数 成为其一部分 即model.parameters()会包含这个parameter<br>        # 从而在参数优化的时候可以自动一起优化<br> <br>    def forward(self, x):<br>        w = self.w<br>        weight = w / (torch.sum(w, dim=0) + self.epsilon)  # 将权重进行归一化<br>        # Fast normalized fusion<br>        x = [weight[0] * x[0], weight[1] * x[1]]<br>        return torch.cat(x, self.d)<br> <br> <br># 三个分支concat操作<br>class BiFPN_Concat3(nn.Module):<br>    def __init__(self, dimension=1):<br>        super(BiFPN_Concat3, self).__init__()<br>        self.d = dimension<br>        self.w = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)<br>        self.epsilon = 0.0001<br> <br>    def forward(self, x):<br>        w = self.w<br>        weight = w / (torch.sum(w, dim=0) + self.epsilon)  # 将权重进行归一化<br>        # Fast normalized fusion<br>        x = [weight[0] * x[0], weight[1] * x[1], weight[2] * x[2]]<br>        return torch.cat(x, self.d)<br><br></code></pre></td></tr></table></figure>

<h1 id="修改yolov5s-yaml"><a href="#修改yolov5s-yaml" class="headerlink" title="修改yolov5s.yaml"></a>修改yolov5s.yaml</h1><p>BiFPN_Concat本质是add操作，不是concat操作，因此，BiFPN_Concat的各个输入层要求大小完全一致（通道数、feature map大小等），因此，这里要修改之前的参数[-1, 13, 6]，来满足这个要求：<br>-1层就是上一层的输出，原来上一层的输出channel数为256，这里改成512<br>13层就是这里[-1, 3, C3, [512, False]], # 13<br>这样修改后，BiFPN_Concat各个输入大小都是[bs,256,40,40]<br>最后BiFPN_Add后面的参数层设置为[256, 256]也就是输入输出channel数都是256</p>
<figure class="highlight plaintext"><figcaption><span>[魔改]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"># YOLOv5 head<br>head:<br>  [[-1, 1, Conv, [512, 1, 1]],<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 6], 1, BiFPN_Concat2, [1]],  # cat backbone P4<br>   [-1, 3, C3, [512, False]],  # 13<br> <br>   [-1, 1, Conv, [256, 1, 1]],<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 4], 1, BiFPN_Concat2, [1]],  # cat backbone P3<br>   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)<br> <br>   [-1, 1, Conv, [256, 3, 2]],<br>   [[-1, 14,6], 1,BiFPN_Concat3, [1]],  # cat head P4<br>   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)<br> <br>   [-1, 1, Conv, [512, 3, 2]],<br>   [[-1, 10], 1, BiFPN_Concat2, [1]],  # cat head P5<br>   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)<br> <br>   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)<br>  ]<br><br></code></pre></td></tr></table></figure>

<p><strong>3.将类名加入进去，修改yolo.py</strong><br>models&#x2F;yolo.py中的parse_model函数中搜索elif m is Concat:语句,在其后面加上BiFPN_Concat相关语句</p>
<figure class="highlight plaintext"><figcaption><span>[魔改]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"># 添加bifpn_concat结构<br>elif m in [Concat, BiFPN_Concat2, BiFPN_Concat3]:<br>    c2 = sum(ch[x] for x in f)<br><br></code></pre></td></tr></table></figure>

<p><strong>4. 修改train.py</strong></p>
<figure class="highlight plaintext"><figcaption><span>[魔改]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">#调用模块<br>from models.common import BiFPN_Concat2, BiFPN_Concat3<br><br>#向优化器器中添加BiFPN的权重参数<br> pg0, pg1, pg2 = [], [], []  # optimizer parameter groups<br>    for k, v in model.named_modules():<br>        # hasattr: 测试指定的对象是否具有给定的属性，返回一个布尔值<br>        if hasattr(v, &#x27;bias&#x27;) and isinstance(v.bias, nn.Parameter):<br>            pg2.append(v.bias)  # biases<br>        if isinstance(v, nn.BatchNorm2d):<br>            pg0.append(v.weight)  # no decay<br>        elif hasattr(v, &#x27;weight&#x27;) and isinstance(v.weight, nn.Parameter):<br>            pg1.append(v.weight)  # apply decay<br> <br>        elif isinstance(v, BiFPN_Concat2) and hasattr(v, &#x27;w&#x27;) and isinstance(v.w, nn.Parameter):<br>            pg1.append(v.w)<br>        elif isinstance(v, BiFPN_Concat3) and hasattr(v, &#x27;w&#x27;) and isinstance(v.w, nn.Parameter):<br>            pg1.append(v.w)<br> <br> <br>    if opt.adam:<br>        optimizer = optim.Adam(pg0, lr=hyp[&#x27;lr0&#x27;], betas=(hyp[&#x27;momentum&#x27;], 0.999))  # adjust beta1 to momentum<br>    else:<br>        optimizer = optim.SGD(pg0, lr=hyp[&#x27;lr0&#x27;], momentum=hyp[&#x27;momentum&#x27;], nesterov=True)<br> <br>    optimizer.add_param_group(&#123;&#x27;params&#x27;: pg1, &#x27;weight_decay&#x27;: hyp[&#x27;weight_decay&#x27;]&#125;)  # add pg1 with weight_decay<br>    optimizer.add_param_group(&#123;&#x27;params&#x27;: pg2&#125;)  # add pg2 (biases)<br>    logger.info(&#x27;Optimizer groups: %g .bias, %g conv.weight, %g other&#x27; % (len(pg2), len(pg1), len(pg0)))<br>    del pg0, pg1, pg2<br></code></pre></td></tr></table></figure>

<p>最后为了方便测试模型配置文件是否正确、模型是否能够正确进行前向传播，以及查看模型的detect层输出，自己就编写了这个函数</p>
<figure class="highlight plaintext"><figcaption><span>[打印模型参数]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">import argparse<br>import os<br>import torch<br>import yaml<br><br>from models.yolo import Model<br><br># 指定要使用的GPU<br>os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0&#x27;<br><br><br>def parse_opt(known=False):<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(&#x27;--cfg&#x27;, type=str,<br>                        default=&#x27;models/yolov5s.yaml&#x27;,<br>                        help=&#x27;model.yaml path&#x27;)<br>    parser.add_argument(&#x27;--hyp&#x27;, type=str, default=&#x27;data/hyps/hyp.scratch-high.yaml&#x27;,<br>                        help=&#x27;hyperparameters path&#x27;)<br><br>    opt = parser.parse_known_args()[0] if known else parser.parse_args()<br>    return opt<br><br><br>def main(opt):<br>    with open(opt.hyp, encoding=&#x27;utf-8&#x27;, errors=&#x27;ignore&#x27;) as f:<br>        hyp = yaml.safe_load(f)  # load hyps dict 字典形式<br><br>    # 如果配置文件中有中文，打开时要加encoding = &#x27;utf-8&#x27;参数<br>    with open(opt.cfg, encoding=&#x27;ascii&#x27;, errors=&#x27;ignore&#x27;) as f:<br>        cfg = yaml.safe_load(f)  # model dict 取到配置文件中每条的信息<br><br>    nc = cfg[&#x27;nc&#x27;]  # 获取数据集的类别数<br>    device = torch.device(&#x27;cuda:0&#x27;) if torch.cuda.is_available() else &#x27;cpu&#x27;<br>    print(f&#x27;device: &#123;device&#125;&#x27;)<br>    # input_img = torch.zeros(size=(1, 3, 1280, 1280))<br>    input_img = torch.zeros(size=(1, 3, 640, 640))<br>    input_img = input_img.to(device, non_blocking=True).float()<br>    print(f&#x27;the model of \&#x27;&#123;opt.cfg&#125;\&#x27; is :&#x27;)<br>    model = Model(opt.cfg, ch=3, nc=nc, anchors=hyp.get(&#x27;anchors&#x27;)).to(device)  # create<br>    output = model(input_img)<br><br>    print(f&#x27;number of detect layers: &#123;len(output)&#125;&#x27;)<br>    print(&#x27;Detect head output: &#x27;)<br>    # print(f&#x27;P2/4: &#123;output[len(output) - 4].shape&#125;&#x27;)<br>    print(f&#x27;P3/8: &#123;output[len(output) - 3].shape&#125;&#x27;)<br>    print(f&#x27;P4/16: &#123;output[len(output) - 2].shape&#125;&#x27;)<br>    print(f&#x27;P5/32: &#123;output[len(output) - 1].shape&#125;&#x27;)<br><br>    &#x27;&#x27;&#x27;<br>    3<br>    torch.Size([1, 3, 80, 80, 85])<br>    torch.Size([1, 3, 40, 40, 85])<br>    torch.Size([1, 3, 20, 20, 85])<br>    &#x27;&#x27;&#x27;<br><br><br>if __name__ == &#x27;__main__&#x27;:<br>    opt = parse_opt()<br>    main(opt)<br><br></code></pre></td></tr></table></figure>

<p>pytorch获取模型某一层参数名及参数值方式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">import os<br>import torch<br>import torch.nn as nn<br><br># 设置GPU<br>os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;1&#x27;<br>device = torch.device(&#x27;cuda:0&#x27;) if torch.cuda.is_available() else &#x27;cpu&#x27;<br><br># 创建模型<br>model = nn.Sequential(nn.Conv2d(3, 16, kernel_size=1),<br>                      nn.Conv2d(16, 3, kernel_size=1))<br>model.to(device)<br><br># 方法一<br># 打印某一层的参数名<br>for name in model.state_dict():<br>    print(name)<br># 直接索引某一层的name来输出该层的参数<br>print(model.state_dict()[&#x27;1.weight&#x27;])<br><br># 方法二<br># 获取模型所有参数名和参数值 存储在list中<br>params = list(model.named_parameters())<br># 分别索引得到某层的名称和参数值<br>print(params[2][0])  # name<br>print(params[2][1].data)  # data<br><br># 方法三<br># 依次遍历模型每一层的参数 存储到dict中<br>params = &#123;&#125;<br>for name, param in model.named_parameters():<br>    params[name] = param.detach().cpu().numpy()<br>print(params[&#x27;0.weight&#x27;])<br><br># 方法四<br># 遍历模型的每一层 查找目标层 输出参数值<br>for layer in model.modules():<br>    # 打印Conv2d层的参数<br>    if (isinstance(layer, nn.Conv2d)):<br>        print(layer.weight)<br><br></code></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>-yolov5 -head -BiFPN</tag>
      </tags>
  </entry>
  <entry>
    <title>decorator</title>
    <url>/myblog/2023/08/10/decorator/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/myblog/css/APlayer.min.css"><script src="/myblog/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/myblog/js/Meting.min.js"></script><h1 id="1、-timer-测量执行时间"><a href="#1、-timer-测量执行时间" class="headerlink" title="1、@timer:测量执行时间"></a>1、@timer:测量执行时间</h1><p>@timer装饰器可以帮助我们跟踪特定函数的执行时间。通过用这个装饰器包装函数，我可以快速识别瓶颈并优化代码的关键部分。</p>
<figure class="highlight plaintext"><figcaption><span>[测量执行时间]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">import time<br> <br>def timer(func):<br>   def wrapper(*args, **kwargs):<br>       start_time = time.time()<br>       result = func(*args, **kwargs)<br>       end_time = time.time()<br>       print(f&quot;&#123;func.__name__&#125; took &#123;end_time - start_time:.2f&#125; seconds to execute.&quot;)<br>       return result<br>   return wrapper<br>@timer<br>def my_data_processing_function():<br>   # Your data processing code here<br><br></code></pre></td></tr></table></figure>

<h1 id="2-log-results-日志输出"><a href="#2-log-results-日志输出" class="headerlink" title="2.@log_results:日志输出"></a>2.@log_results:日志输出</h1><p>在运行复杂的数据分析时，跟踪每个函数的输出变得至关重要。@log_results装饰器可以帮助我们记录函数的结果，以便于调试和监控</p>
<figure class="highlight plaintext"><figcaption><span>[日志输出]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">def log_results(func):<br>    def wrapper(*args, **kwargs):<br>        result = func(*args, **kwargs)<br>        with open(&quot;results.log&quot;, &quot;a&quot;) as log_file:<br>            log_file.write(f&quot;&#123;func.__name__&#125; - Result: &#123;result&#125;\n&quot;)<br>        return result<br> <br>    return wrapper<br> @log_results<br>def calculate_metrics(data):<br>   # Your metric calculation code here<br><br></code></pre></td></tr></table></figure>

<h1 id="3-suppress-errors-优雅的错误处理"><a href="#3-suppress-errors-优雅的错误处理" class="headerlink" title="3.@suppress_errors:优雅的错误处理"></a>3.@suppress_errors:优雅的错误处理</h1><p>@suppress_errors装饰器可以优雅地处理异常并继续执行,可以避免隐藏严重错误，还可以进行错误的详细输出，便于调试.</p>
<figure class="highlight plaintext"><figcaption><span>[错误处理]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">def suppress_errors(func):<br>    def wrapper(*args, **kwargs):<br>        try:<br>            return func(*args, **kwargs)<br>        except Exception as e:<br>            print(f&quot;Error in &#123;func.__name__&#125;: &#123;e&#125;&quot;)<br>            return None<br> <br>    return wrapper<br>@suppress_errors<br>def preprocess_data(data):<br>   # Your data preprocessing code here<br><br></code></pre></td></tr></table></figure>

<h1 id="4-debug-调试变得更容易"><a href="#4-debug-调试变得更容易" class="headerlink" title="4.@debug:调试变得更容易"></a>4.@debug:调试变得更容易</h1><p>调试复杂的代码可能非常耗时。@debug装饰器可以打印函数的输入参数和它们的值，以便于调试:</p>
<figure class="highlight plaintext"><figcaption><span>[调试]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">def debug(func):<br>    def wrapper(*args, **kwargs):<br>        print(f&quot;Debugging &#123;func.__name__&#125; - args: &#123;args&#125;, kwargs: &#123;kwargs&#125;&quot;)<br>        return func(*args, **kwargs)<br> <br>    return wrapper<br>@debug<br>def complex_data_processing(data, threshold=0.5):<br>   # Your complex data processing code here<br><br></code></pre></td></tr></table></figure>


<blockquote><blockquote>
<p>1.@timer:测量执行时间<br>2.@log_results:日志输出<br>3.@suppress_errors:优雅的错误处理<br>4.@debug:调试变得更容易</p>
</blockquote>
<footer><strong>数据STUDIO</strong><cite>- source [https://mp.weixin.qq.com/s/JFaH_GqOFMARnzyWqV2TFQ] [10个简单但很有用的Python装饰器]</cite></footer></blockquote>
]]></content>
      <tags>
        <tag>-python -decorator</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/myblog/2023/10/09/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/myblog/css/APlayer.min.css"><script src="/myblog/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/myblog/js/Meting.min.js"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>(三)yolov5检测头head魔改之CBAM，ODConv|添加注意力</title>
    <url>/myblog/2023/10/12/yolov5_head_CBAM/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/myblog/css/APlayer.min.css"><script src="/myblog/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/myblog/js/Meting.min.js"></script><h1 id="1-CBAM介绍"><a href="#1-CBAM介绍" class="headerlink" title="1.CBAM介绍"></a>1.CBAM介绍</h1><p>论文题目：《CBAM: Convolutional Block Attention Module》<br>论文地址：  <a href="https://arxiv.org/pdf/1807.06521.pdf">https://arxiv.org/pdf/1807.06521.pdf</a><br>实验证明，将CBAM注意力模块嵌入到YOLOv5网络中，有利于解决原始网络无注意力偏好的问题。主要在分类问题中比较明显</p>
<p><img src="https://s2.loli.net/2023/10/12/wUIdgiNVbkvB6ap.png" alt="image.png"></p>
<p>CBAM注意力结构基本原理：从上图明显可以看到， CBAM一共包含2个独立的子模块， 通道注意力模块（Channel Attention Module，CAM) 和空间注意力模块（Spartial Attention Module，SAM) ，分别进行通道与空间维度上的注意力特征融合。 这样不只能够节约参数和计算力，并且保证了其能够做为即插即用的模块集成到现有的网络架构中去。</p>
<p>那对应YOLOv5结合CBAM需要修改哪些地方：</p>
<h1 id="2-common-py中加入CBAM代码"><a href="#2-common-py中加入CBAM代码" class="headerlink" title="2.common.py中加入CBAM代码"></a>2.common.py中加入CBAM代码</h1><figure class="highlight plaintext"><figcaption><span>[CBAM]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext">class ChannelAttentionModule(nn.Module):<br>    def __init__(self, c1, reduction=16):<br>        super(ChannelAttentionModule, self).__init__()<br>        mid_channel = c1 // reduction<br>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<br>        self.max_pool = nn.AdaptiveMaxPool2d(1)<br>        self.shared_MLP = nn.Sequential(<br>            nn.Linear(in_features=c1, out_features=mid_channel),<br>            nn.ReLU(),<br>            nn.Linear(in_features=mid_channel, out_features=c1)<br>        )<br>        self.sigmoid = nn.Sigmoid()<br>        #self.act=SiLU()<br>    def forward(self, x):<br>        avgout = self.shared_MLP(self.avg_pool(x).view(x.size(0),-1)).unsqueeze(2).unsqueeze(3)<br>        maxout = self.shared_MLP(self.max_pool(x).view(x.size(0),-1)).unsqueeze(2).unsqueeze(3)<br>        return self.sigmoid(avgout + maxout)<br>class SpatialAttentionModule(nn.Module):<br>    def __init__(self):<br>        super(SpatialAttentionModule, self).__init__()<br>        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3) <br>        #self.act=SiLU()<br>        self.sigmoid = nn.Sigmoid()<br>    def forward(self, x):<br>        avgout = torch.mean(x, dim=1, keepdim=True)<br>        maxout, _ = torch.max(x, dim=1, keepdim=True)<br>        out = torch.cat([avgout, maxout], dim=1)<br>        out = self.sigmoid(self.conv2d(out))<br>        return out<br> <br>class CBAM(nn.Module):<br>    def __init__(self, c1,c2):<br>        super(CBAM, self).__init__()<br>        self.channel_attention = ChannelAttentionModule(c1)<br>        self.spatial_attention = SpatialAttentionModule()<br> <br>    def forward(self, x):<br>        out = self.channel_attention(x) * x<br>        out = self.spatial_attention(out) * out<br>        return out<br></code></pre></td></tr></table></figure>


<h1 id="3-在yolo-py文件中添加对应的CBAM"><a href="#3-在yolo-py文件中添加对应的CBAM" class="headerlink" title="3.在yolo.py文件中添加对应的CBAM"></a>3.在yolo.py文件中添加对应的CBAM</h1><figure class="highlight plaintext"><figcaption><span>[yolo.py]</span></figcaption><table><tr><td class="code"><pre><code class="hljs plaintext"> if m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, <br>DWConv, MixConv2d, Focus, CrossConv, BottleneckCSP, C3, C3TR, CBAM]:      <br></code></pre></td></tr></table></figure>

<h1 id="4-在yolov5s-yaml文件中添加CBAM"><a href="#4-在yolov5s-yaml文件中添加CBAM" class="headerlink" title="4.在yolov5s.yaml文件中添加CBAM"></a>4.在yolov5s.yaml文件中添加CBAM</h1><p>根据实际训练效果，在配置文件中的C3模块后面适当添加CBAM注意力模块，过程中注意通道数和网络层数的变化（注：不同添加位置效果可能不大一样），例如下面作为参考：在最后检测层上面添加CBAM，其中BiFPND，BiFPNT分别是二分支与三分支的BiFPN</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">#Head<br>head:<br>  [[-1, 1, Conv, [512, 1, 1]],                     #14<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],     #15<br>   [[-1, 9], 1, BiFPND, [256, 256]],               #16 <br>   [-1, 3, C3, [512, False]],                      #17<br> <br>   [-1, 1, Conv, [256, 1, 1]],                     #18<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],     #19<br>   [[-1, 6], 1, BiFPND, [128, 128]],               #20 <br>   [-1, 3, C3, [256, False]],                      #21 <br> <br>   [-1, 1, Conv, [512, 3, 2]],                     #22<br>   [[-1, 17, 9], 1, BiFPNT, [256, 256]],          #23<br>   [-1, 3, C3, [512, False]],                      #24 <br> <br>   [-1, 1, Conv, [512, 3, 2]],                     #25<br>   [[-1, 14], 1, BiFPND, [256, 256]],              #26<br>   [-1, 3, C3, [1024, False]],                     #27 <br>   [-1, 1, CBAM, [1024]],                            #28<br> <br>   [[21, 24, 28], 1, Detect, [nc, anchors]],       #29 Detect<br>  ]<br></code></pre></td></tr></table></figure>

<h1 id="其它注意力SE，ECA与CSBAM区别"><a href="#其它注意力SE，ECA与CSBAM区别" class="headerlink" title="其它注意力SE，ECA与CSBAM区别"></a>其它注意力SE，ECA与CSBAM区别</h1><p>注意力机制顾名思义就是通过对感兴趣的区域提升更多的注意力，尽可能的抑制不感兴趣的区域在图像分割中的作用。深度学习CNN中可以将注意力机制分为通道注意力和空间注意力两种，通道注意力是确定不同通道之间的权重关系，提升重点通道的权重，抑制作用不大的通道，空间注意力是确定空间邻域不同像素之间的权重关系，提升重点区域像素的权重，让算法更多的关注我们需要的研究区域，减小非必要区域的权重。</p>
<p><strong>一、SE (Squeeze and Excitation）注意力机制</strong><br>SE注意力机制是通道注意力模式下的一种确定权重的方法，它通过在不同通道间分配权重达到主次优先的目的。如下图所示，为SE注意力机制的结构图。</p>
<p>该结构主要分为以下三个方面：</p>
<p>①：通过将特征图进行Squeeze(压缩)，该步骤是通过全局平均池化把特征图从大小为（N,C,H,W)转换为（N,C,1,1)，这样就达到了全局上下文信息的融合。</p>
<p>②：Excitation操作，该步骤使用两个全连接层，通过全连接层之间的非线性添加模型的复杂度，达到确定不同通道之间的权重作用，其中第一个全连接层使用ReLU激活函数，第二个全连接层采用Sigmoid激活函数，为了将权重中映射到（0，1）之间。值得注意的是，为了减少计算量进行降维处理，将第一个全连接的输出采用输入的1&#x2F;4或者1&#x2F;16。</p>
<p>③：将reshape过后的权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。</p>
<p><img src="https://s2.loli.net/2023/10/12/IPld8LETkFc6B5w.png" alt="image.png"></p>
<p>具体PyTorch实现代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">import torch<br>import torch.nn as nn<br><br><br>class Se(nn.Module):<br>    def __init__(self,in_channel,reduction=16):<br>        super(Se, self).__init__()<br>        self.pool=nn.AdaptiveAvgPool2d(output_size=1)<br>        self.fc=nn.Sequential(<br>            nn.Linear(in_features=in_channel,out_features=in_channel//reduction,bias=False),<br>            nn.ReLU(),<br>            nn.Linear(in_features=in_channel//reduction,out_features=in_channel,bias=False),<br>            nn.Sigmoid()<br>        )<br><br>    def forward(self,x):<br>        out=self.pool(x)<br>        out=self.fc(out.view(out.size(0),-1))<br>        out=out.view(x.size(0),x.size(1),1,1)<br>        return out*x<br></code></pre></td></tr></table></figure>

<p><strong>二、ECA（Efficient Channel Attention）</strong><br>ECA注意力机制也是通道注意力的一种方法，该算法是在SE算法的基础上做出了一定的改进，首先ECA作者认为SE虽然全连接的降维可以降低模型的复杂度，但是破坏了通道与其权重之间的直接对应关系，先降维后升维，这样权重和通道的对应关系是间接的，基于上述，作者提出一维卷积的方法，避免了降维对数据的影响。</p>
<p>该结构主要分为以下三个方面：</p>
<p>①：通过将特征图进行Squeeze(压缩)，该步骤是通过全局平均池化把特征图从大小为（N,C,H,W)转换为（N,C,1,1)，这样就达到了全局上下文信息的融合，该步骤和SE一样。</p>
<p>②：计算自适应卷积核的大小，<br> ，其中C为输入的通道数，b&#x3D;1，<br> &#x3D;2，并采用一维卷积计算通道的权重，最后采用Sigmoid激活函数将权重映射在（0-1）之间。</p>
<p>③：将reshape过后的权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。</p>
<p><img src="https://s2.loli.net/2023/10/12/NhXnP3lx5a7FTvG.png" alt="image.png"></p>
<p>具体PyTorch实现代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">import torch<br>import torch.nn as nn<br>import math<br>class ECA(nn.Module):<br>    def __init__(self,in_channel,gamma=2,b=1):<br>        super(ECA, self).__init__()<br>        k=int(abs((math.log(in_channel,2)+b)/gamma))<br>        kernel_size=k if k % 2 else k+1<br>        padding=kernel_size//2<br>        self.pool=nn.AdaptiveAvgPool2d(output_size=1)<br>        self.conv=nn.Sequential(<br>            nn.Conv1d(in_channels=1,out_channels=1,kernel_size=kernel_size,padding=padding,bias=False),<br>            nn.Sigmoid()<br>        )<br><br>    def forward(self,x):<br>        out=self.pool(x)<br>        out=out.view(x.size(0),1,x.size(1))<br>        out=self.conv(out)<br>        out=out.view(x.size(0),x.size(1),1,1)<br>        return out*x<br></code></pre></td></tr></table></figure>

<p><strong>三、CBAM（Convolutional Block Attention Module）</strong><br>CBAM注意力机制是一种将通道与空间注意力机制相结合的算法模型，算法整体结构如图3所示，输入特征图先进行通道注意力机制再进行空间注意力机制操作，最后输出，这样从通道和空间两个方面达到了强化感兴趣区域的目的。</p>
<p>通道结构主要分为以下三个方面：</p>
<p>①：通过将特征图进行Squeeze(压缩)，该步骤分别采用全局平均池化和全局最大池化把特征图从大小为（N,C,H,W)转换为（N,C,1,1)，这样就达到了全局上下文信息的融合。</p>
<p>②：分别将全局最大池化和全局平均池化结果进行MLP（多层感知机）操作，MLP在这里定义与SE的操作一样，为两层全连接层，中间采用ReLU激活，最后将两者相加后利用Sigmoid函数激活。</p>
<p>③：将reshape过后的权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。</p>
<p>空间结构主要分为以下三个方面：</p>
<p>①：将上述通道注意力操作的结果，分别在通道维度上进行最大池化和平均池化，即将经过通道注意力机制的特征图从（N,C,H,W)转换为（N,1,H,W），达到融合不同通道的信息的效果，然后在通道维度上将最大池化与平均池化结果叠加起来，即采用torch.cat()。</p>
<p>②：将叠加后2个通道的结果做卷积运算，输出通道为1，卷积核大小为7，最后将输出结果采用Sigmoid函数激活。</p>
<p>③：将权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。</p>
<p><img src="https://s2.loli.net/2023/10/12/wGMIACQ87LpNdhg.png" alt="image.png"></p>
<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">import torch<br>import torch.nn as nn<br>import math<br>class CBAM(nn.Module):<br>    def __init__(self,in_channel,reduction=16,kernel_size=7):<br>        super(CBAM, self).__init__()<br>        #通道注意力机制<br>        self.max_pool=nn.AdaptiveMaxPool2d(output_size=1)<br>        self.avg_pool=nn.AdaptiveAvgPool2d(output_size=1)<br>        self.mlp=nn.Sequential(<br>            nn.Linear(in_features=in_channel,out_features=in_channel//reduction,bias=False),<br>            nn.ReLU(),<br>            nn.Linear(in_features=in_channel//reduction,out_features=in_channel,bias=False)<br>        )<br>        self.sigmoid=nn.Sigmoid()<br>        #空间注意力机制<br>        self.conv=nn.Conv2d(in_channels=2,out_channels=1,kernel_size=kernel_size ,stride=1,padding=kernel_size//2,bias=False)<br><br>    def forward(self,x):<br>        #通道注意力机制<br>        maxout=self.max_pool(x)<br>        maxout=self.mlp(maxout.view(maxout.size(0),-1))<br>        avgout=self.avg_pool(x)<br>        avgout=self.mlp(avgout.view(avgout.size(0),-1))<br>        channel_out=self.sigmoid(maxout+avgout)<br>        channel_out=channel_out.view(x.size(0),x.size(1),1,1)<br>        channel_out=channel_out*x<br>        #空间注意力机制<br>        max_out,_=torch.max(channel_out,dim=1,keepdim=True)<br>        mean_out=torch.mean(channel_out,dim=1,keepdim=True)<br>        out=torch.cat((max_out,mean_out),dim=1)<br>        out=self.sigmoid(self.conv(out))<br>        out=out*channel_out<br>        return out<br></code></pre></td></tr></table></figure>

<h1 id="另一利用注意力模块的ODConv：即插即用的动态卷积"><a href="#另一利用注意力模块的ODConv：即插即用的动态卷积" class="headerlink" title="另一利用注意力模块的ODConv：即插即用的动态卷积"></a>另一利用注意力模块的ODConv：即插即用的动态卷积</h1><p>一定程度上讲，ODConv可以视作CondConv的延续，将CondConv中一个维度上的动态特性进行了扩展，同时了考虑了空域、输入通道、输出通道等维度上的动态性，故称之为全维度动态卷积。ODConv通过并行策略采用多维注意力机制沿核空间的四个维度学习互补性注意力。作为一种“即插即用”的操作，它可以轻易的嵌入到现有CNN网络中。ImageNet分类与COCO检测任务上的实验验证了所提ODConv的优异性：即可提升大模型的性能，又可提升轻量型模型的性能</p>
<p>动态卷积这几年研究的非常多了，比较知名的有谷歌的CondConv，旷视科技的WeightNet、MSRA的DynamicConv、华为的DyNet、商汤的CARAFE等<br>常规卷积只有一个静态卷积核且与输入样本无关。对于动态卷积来说，它对多个卷积核进行线性加权，而加权值则与输入有关，这就使得动态卷积具有输入依赖性。它可以描述如下：<br><img src="https://s2.loli.net/2023/10/12/nTFJOcZfz25ML3K.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2023/10/12/CgOm5r9Lqs7TY8K.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2023/10/12/mnjYfTA8EikboZx.png" alt="image.png"></p>
<p>ODConv比一般的动态卷积效果都要好，比一般的注意力机制效果也要好，其实动态卷积与注意力机制是从模块中不同角度进行解析，动态卷积是相比较于常规卷积，最终需要与输入相乘，与输入有关，而常规卷积与输入无关；注意力机制是相对于特征图的四个维度比较的，也即卷积方式不一样，以往是直接进行特征图与卷积核进行卷积来获取特征，而注意力机制就是将特征图分别对四个通道进行压缩，来获取对比某一通道在特征图的影响力，再分别与卷积核进行线性加权，最后与输入相乘。因此像动态卷积与注意力其实一个东西，只是ODConv是考虑到所有维度的注意力，因此效果最好</p>
<p>参考链接：<a href="https://zhuanlan.zhihu.com/p/468466504">https://zhuanlan.zhihu.com/p/468466504</a></p>
<h1 id="5-在yolov5中添加ODConv"><a href="#5-在yolov5中添加ODConv" class="headerlink" title="5.在yolov5中添加ODConv"></a>5.在yolov5中添加ODConv</h1><p><strong>1.首先在common.py文件中添加代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext"><br>import torch<br>import torch.nn as nn<br>import torch.nn.functional as F<br>import torch.autograd<br><br>class ODConv(nn.Sequential):<br>    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1, norm_layer=nn.BatchNorm2d,<br>                 reduction=0.0625, kernel_num=1):<br>        padding = (kernel_size - 1) // 2<br>        super(ODConv, self).__init__(<br>            ODConv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups,<br>                     reduction=reduction, kernel_num=kernel_num),<br>            norm_layer(out_planes),<br>            nn.SiLU()<br>        )<br><br>class Attention(nn.Module):<br>    def __init__(self, in_planes, out_planes, kernel_size, <br>    groups=1, <br>    reduction=0.0625, <br>    kernel_num=4, <br>    min_channel=16):<br>        super(Attention, self).__init__()<br>        attention_channel = max(int(in_planes * reduction), min_channel)<br>        self.kernel_size = kernel_size<br>        self.kernel_num = kernel_num<br>        self.temperature = 1.0<br><br>        self.avgpool = nn.AdaptiveAvgPool2d(1)<br>        self.fc = nn.Conv2d(in_planes, attention_channel, 1, bias=False)<br>        self.bn = nn.BatchNorm2d(attention_channel)<br>        self.relu = nn.ReLU(inplace=True)<br><br>        self.channel_fc = nn.Conv2d(attention_channel, in_planes, 1, bias=True)<br>        self.func_channel = self.get_channel_attention<br><br>        if in_planes == groups and in_planes == out_planes:  # depth-wise convolution<br>            self.func_filter = self.skip<br>        else:<br>            self.filter_fc = nn.Conv2d(attention_channel, out_planes, 1, bias=True)<br>            self.func_filter = self.get_filter_attention<br><br>        if kernel_size == 1:  # point-wise convolution<br>            self.func_spatial = self.skip<br>        else:<br>            self.spatial_fc = nn.Conv2d(attention_channel, kernel_size * kernel_size, 1, bias=True)<br>            self.func_spatial = self.get_spatial_attention<br><br>        if kernel_num == 1:<br>            self.func_kernel = self.skip<br>        else:<br>            self.kernel_fc = nn.Conv2d(attention_channel, kernel_num, 1, bias=True)<br>            self.func_kernel = self.get_kernel_attention<br>        self.bn_1 = nn.LayerNorm([attention_channel,1,1])<br>        self._initialize_weights()<br><br>    def _initialize_weights(self):<br>        for m in self.modules():<br>            if isinstance(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, mode=&#x27;fan_out&#x27;, nonlinearity=&#x27;relu&#x27;)<br>                if m.bias is not None:<br>                    nn.init.constant_(m.bias, 0)<br>            if isinstance(m, nn.BatchNorm2d):<br>                nn.init.constant_(m.weight, 1)<br>                nn.init.constant_(m.bias, 0)<br><br>    def update_temperature(self, temperature):<br>        self.temperature = temperature<br><br>    @staticmethod<br>    def skip(_):<br>        return 1.0<br><br>    def get_channel_attention(self, x):<br>        channel_attention = torch.sigmoid(self.channel_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)<br>        return channel_attention<br><br>    def get_filter_attention(self, x):<br>        filter_attention = torch.sigmoid(self.filter_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)<br>        return filter_attention<br><br>    def get_spatial_attention(self, x):<br>        spatial_attention = self.spatial_fc(x).view(x.size(0), 1, 1, 1, self.kernel_size, self.kernel_size)<br>        spatial_attention = torch.sigmoid(spatial_attention / self.temperature)<br>        return spatial_attention<br><br>    def get_kernel_attention(self, x):<br>        kernel_attention = self.kernel_fc(x).view(x.size(0), -1, 1, 1, 1, 1)<br>        kernel_attention = F.softmax(kernel_attention / self.temperature, dim=1)<br>        return kernel_attention<br><br>    def forward(self, x):<br>        x = self.avgpool(x)<br>        x = self.fc(x)<br>        x = self.bn_1(x)<br>        x = self.relu(x)<br>        return self.func_channel(x), self.func_filter(x), self.func_spatial(x), self.func_kernel(x)<br><br>class ODConv2d(nn.Module):<br>    def __init__(self, <br>    in_planes, <br>    out_planes, <br>    kernel_size=3, <br>    stride=1, <br>    padding=0, <br>    dilation=1, <br>    groups=1,<br>    reduction=0.0625, <br>    kernel_num=1):<br>        super(ODConv2d, self).__init__()<br>        self.in_planes = in_planes<br>        self.out_planes = out_planes<br>        self.kernel_size = kernel_size<br>        self.stride = stride<br>        self.padding = padding<br>        self.dilation = dilation<br>        self.groups = groups<br>        self.kernel_num = kernel_num<br>        self.attention = Attention(in_planes, out_planes, kernel_size, groups=groups,<br>                                   reduction=reduction, kernel_num=kernel_num)<br>        self.weight = nn.Parameter(torch.randn(kernel_num, out_planes, in_planes//groups, kernel_size, kernel_size),<br>                                   requires_grad=True)<br>        self._initialize_weights()<br><br>        if self.kernel_size == 1 and self.kernel_num == 1:<br>            self._forward_impl = self._forward_impl_pw1x<br>        else:<br>            self._forward_impl = self._forward_impl_common<br><br>    def _initialize_weights(self):<br>        for i in range(self.kernel_num):<br>            nn.init.kaiming_normal_(self.weight[i], mode=&#x27;fan_out&#x27;, nonlinearity=&#x27;relu&#x27;)<br><br>    def update_temperature(self, temperature):<br>        self.attention.update_temperature(temperature)<br><br>    def _forward_impl_common(self, x):<br><br>        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)<br>        batch_size, in_planes, height, width = x.size()<br>        x = x * channel_attention<br>        x = x.reshape(1, -1, height, width)<br>        aggregate_weight = spatial_attention * kernel_attention * self.weight.unsqueeze(dim=0)<br>        aggregate_weight = torch.sum(aggregate_weight, dim=1).view(<br>            [-1, self.in_planes // self.groups, self.kernel_size, self.kernel_size])<br>        output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,<br>                          dilation=self.dilation, groups=self.groups * batch_size)<br>        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))<br>        output = output * filter_attention<br>        return output<br><br>    def _forward_impl_pw1x(self, x):<br>        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)<br>        x = x * channel_attention<br>        output = F.conv2d(x, weight=self.weight.squeeze(dim=0), bias=None, stride=self.stride, padding=self.padding,<br>                          dilation=self.dilation, groups=self.groups)<br>        output = output * filter_attention<br>        return output<br><br>    def forward(self, x):<br>        return self._forward_impl(x)<br><br></code></pre></td></tr></table></figure>

<p><strong>2.在yolo.py文件中添加对应的CBAM</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">elif m in [ODConv]:<br>    c1, c2 = ch[f], args[0]<br>    if c2 != no:  # if not output<br>        c2 = make_divisible(c2 * gw, 8)<br><br>    args = [c1, c2, *args[1:]]<br></code></pre></td></tr></table></figure>

<p>在yolov5.yaml文件中则需要除了backbone网络中第一个卷积之外，其它卷积都可以替换为ODConv<br>例如替换head中一个卷积conv</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs plaintext">head:<br>  [[-1, 1, Conv, [512, 1, 1]],<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 6], 1, Concat, [1]],  # cat backbone P4<br>   [-1, 3, C3, [512, False]],  # 13<br><br>   [-1, 1, Conv, [256, 1, 1]],<br>   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],<br>   [[-1, 4], 1, Concat, [1]],  # cat backbone P3<br>   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)<br><br>   [-1, 1, Conv, [256, 3, 2]],<br>   [[-1, 14], 1, Concat, [1]],  # cat head P4<br>   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)<br><br>   [-1, 1, ODConv, [512, 3, 2]],<br>   [[-1, 10], 1, Concat, [1]],  # cat head P5<br>   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)<br><br>   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)<br>  ]<br></code></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>-yolov5 -head -CBAM -ODConv</tag>
      </tags>
  </entry>
</search>
